{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: \t\t2.0.9\n",
      "Scikit version: \t0.19.1\n",
      "TensorFlow version: \t1.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras, os, pickle, re, sklearn, string, tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adadelta, adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils  import plot_model\n",
    "\n",
    "print('Keras version: \\t\\t%s' % keras.__version__)\n",
    "print('Scikit version: \\t%s' % sklearn.__version__)\n",
    "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tika import parser\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData_others():\n",
    "    label_others=[]\n",
    "    i=0\n",
    "    for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others\"):\n",
    "        try:\n",
    "            print (i, file)\n",
    "            parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/\"+file)\n",
    "\n",
    "            tech_contents=clean_doc(parsedPDF['content'])\n",
    "            #Data = resume_contents.encode('utf-8')    \n",
    "            label_others.append((tech_contents,'others'))\n",
    "        except UnicodeEncodeError:\n",
    "            print ('Unicode error:', file)\n",
    "        i=i+1\n",
    "    #print (label_resume)\n",
    "    return(label_others)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData_webdesigner():\n",
    "    label_webdesigner=[]\n",
    "    i=1\n",
    "    for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner\"):\n",
    "        try:\n",
    "            print (i, file)\n",
    "            parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/\"+file)\n",
    "        \n",
    "            it_contents=clean_doc(parsedPDF['content'])\n",
    "       \n",
    "            \n",
    "            label_webdesigner.append((it_contents, 'webdesigner'))\n",
    "        except UnicodeEncodeError:\n",
    "            print ('Unicode error:', file)\n",
    "        i=i+1\n",
    "            \n",
    "    return(label_webdesigner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods:\n",
    "        - Lowercase\n",
    "        - Removing whitespaces\n",
    "        - Removing stopwords\n",
    "        - Removing punctuations\n",
    "        \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "   \n",
    "    tokens = doc.translate({ord(c):\"\" for c in \"\\u200b\\uf020\\u2028\\xa0\\uf0e0\\uf095\\uf041\\uf0e1\\uf0b7\\xad\"})\n",
    "    tokens = tokens.translate({ord(c):\" \" for c in \"[):,·](;•●■♦▪\"})\n",
    "    tokens = tokens.translate({ord(c):\"f\" for c in \"�\"})\n",
    "    \n",
    "    # Removing multiple whitespaces\n",
    "    tokens = re.sub(r\"\\?\", \" \\? \", tokens)\n",
    "    \n",
    "    # Split in tokens\n",
    "    tokens = tokens.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RajibDebnathProfile.pdf\n",
      "1 NeriinnGopuProfile(1).pdf\n",
      "2 NataliaMolchanovaProfile.pdf\n",
      "3 SimonaTedescoProfile.pdf\n",
      "4 3d Character Animator Resume.doc\n",
      "5 SagaciousDesignProfile.pdf\n",
      "6 LaurenHamillProfile.pdf\n",
      "7 CostantinoMontanariProfile.pdf\n",
      "8 IlariaAlquatiProfile(1).pdf\n",
      "9 Martina GinevraAlbanoProfile.pdf\n",
      "10 LorenzoDe AngelisProfile.pdf\n",
      "11 AtaHaghdarProfile.pdf\n",
      "12 LorenzoDe AngelisProfile(2).pdf\n",
      "13 ValerioBailo ModestiProfile.pdf\n",
      "14 SoniaDonizProfile.pdf\n",
      "15 StevaDokmanacProfile.pdf\n",
      "16 KimberlyPelkeyProfile.pdf\n",
      "17 SimonaMuscogiuriProfile.pdf\n",
      "18 TureNicotraProfile.pdf\n",
      "19 Ramana RaoPanduriProfile.pdf\n",
      "20 ImmaPezzellaProfile.pdf\n",
      "21 ZenithJ JosephProfile.pdf\n",
      "22 AlexFreiProfile.pdf\n",
      "23 MariaMarangoloProfile.pdf\n",
      "24 REJIDHESIProfile.pdf\n",
      "25 KiranMohodProfile.pdf\n",
      "26 NicolaCredentinoProfile.pdf\n",
      "27 SimoneAlbieriProfile.pdf\n",
      "28 SaraRiscicaProfile.pdf\n",
      "29 Suri.Profile.pdf\n",
      "30 GianlucaBaldacciniProfile.pdf\n",
      "31 Creative Resume.pdf\n",
      "32 RobertaAnnicchiaricoProfile.pdf\n",
      "33 IlariaAlquatiProfile.pdf\n",
      "34 LaviniaBizzarriProfile.pdf\n",
      "35 SimonaCorrentiProfile.pdf\n",
      "36 GianlucaBaldacciniProfile(1).pdf\n",
      "37 GaspareNicosiaProfile.pdf\n",
      "38 Susan ClodiaTonsoProfile.pdf\n",
      "39 IlariaPerversiProfile.pdf\n",
      "40 OliviaWhittakerProfile.pdf\n",
      "41 GiusyDe vitoProfile.pdf\n",
      "42 AlessandroPuricellaProfile.pdf\n",
      "43 Manuele R.PennisiProfile.pdf\n",
      "44 Francesca RomanaRossiProfile.pdf\n",
      "45 AliceCasiraghiProfile.pdf\n",
      "46 GianmarcoIannilliProfile(1).pdf\n",
      "47 Susan ClodiaTonsoProfile(1).pdf\n",
      "48 Professional Graphic Designer Resume in Word.doc\n",
      "49 FedericoBianchiniProfile(1).pdf\n",
      "50 Professional Animator Resume.docx\n",
      "51 GiuliaCappellettiProfile.pdf\n",
      "52 AlexeiAltregoProfile.pdf\n",
      "53 RosaMaiettaProfile.pdf\n",
      "54 ChiaraLamieriProfile.pdf\n",
      "55 BenedettaGoliziaProfile.pdf\n",
      "56 AndreaUciniProfile.pdf\n",
      "57 Elise Chevry,Les Ephelides DesignProfile.pdf\n",
      "58 LauraTaylorProfile.pdf\n",
      "59 Tha'erMrianProfile.pdf\n",
      "60 vinaychopraProfile.pdf\n",
      "61 GianmarcoIannilliProfile.pdf\n",
      "62 Character Animator Resume .doc\n",
      "63 BradBlahnikProfile.pdf\n",
      "64 MehrdadRezaeiProfile.pdf\n",
      "65 AnitaVeronaProfile.pdf\n",
      "66 NorbertoLoardiProfile.pdf\n",
      "67 GopakumarFlash AnimatorProfile.pdf\n",
      "68 RajeshTiwariProfile.pdf\n",
      "69 DonatellaRomanoProfile.pdf\n",
      "70 FedericoFioreProfile.pdf\n",
      "71 Freelance Graphic Designer Resume Template.doc\n",
      "72 giuliapirozziProfile.pdf\n",
      "73 ValerioBertoliProfile.pdf\n",
      "74 SofiaOrsiProfile.pdf\n",
      "75 CarlaMaglioneProfile.pdf\n",
      "76 FedericoBianchiniProfile.pdf\n",
      "77 GiuseppeGiuntaProfile.pdf\n",
      "78 FedericaDel FiaccoProfile.pdf\n",
      "79 SagaciousDesignProfile(1).pdf\n",
      "80 StefanoParisiProfile.pdf\n",
      "81 GaiaPiazzaProfile.pdf\n",
      "82 AndreaScandellaProfile.pdf\n",
      "83 MilenaGajovicProfile.pdf\n",
      "84 KarinaSotskayaProfile.pdf\n",
      "85 SilviaRomeoProfile.pdf\n",
      "86 LorenzoDe AngelisProfile(1).pdf\n",
      "87 DonatoNardulliProfile.pdf\n",
      "88 sachinbhaiProfile.pdf\n",
      "89 TarekShalabyProfile.pdf\n",
      "90 Corporate Graphic Designer.pdf\n",
      "91 CristinaDe LisoProfile.pdf\n",
      "92 MarcoCiminoProfile.pdf\n",
      "93 ShubhamSodaniProfile.pdf\n",
      "94 Graphic Assistant.docx\n",
      "95 JyotiBaghelProfile.pdf\n",
      "96 DavideScarpantonioProfile.pdf\n",
      "97 2D Animator Resume.doc\n",
      "98 Susan ClodiaTonsoProfile(2).pdf\n",
      "99 Entry Level Graphic Designer Resume.doc\n",
      "100 JacquiWilsonProfile.pdf\n",
      "101 AndreaVitaleProfile.pdf\n",
      "102 SalvatorePiscopoProfile.pdf\n",
      "103 ChloeGowerProfile.pdf\n",
      "104 KarolinaBednorzProfile.pdf\n",
      "105 DeepeshBarjatyaProfile.pdf\n",
      "106 DeepakHargaonkarProfile.pdf\n",
      "107 Professional Graphic Designer.pdf\n",
      "108 SholatAliProfile.pdf\n",
      "109 AnnaDe MezzoProfile.pdf\n",
      "110 AntonioIncorvaiaProfile.pdf\n",
      "111 Flash Animator Resume.pdf\n",
      "112 MargheritaBrustiaProfile.pdf\n",
      "113 GianlucaMalimpensaProfile.pdf\n",
      "114 SalemHaddadProfile.pdf\n",
      "115 SilviaScaramucciProfile.pdf\n",
      "116 ManuelMaffezziniProfile.pdf\n",
      "117 ShangningWangProfile.pdf\n",
      "118 JennyLloydProfile.pdf\n",
      "119 GiuliaSonninoProfile(1).pdf\n",
      "120 martamunte vidalProfile.pdf\n",
      "121 AndiB.Profile.pdf\n",
      "122 ClaudiaBrambillaProfile.pdf\n",
      "123 Graphic Designer Skills Resume.pdf\n",
      "124 Sample Junior Graphic Designer Resume .doc\n",
      "125 Senior Graphic Designer.pdf\n",
      "126 RiccardoGiordanoProfile.pdf\n",
      "127 ChiaraPalladinoProfile.pdf\n",
      "128 ShoshanaPortmanProfile.pdf\n",
      "129 gireeshguptaProfile.pdf\n",
      "130 EmmaAllsupProfile.pdf\n",
      "131 FilippoTestaProfile.pdf\n",
      "132 GiuliaSonninoProfile.pdf\n",
      "133 ElisaZorziProfile.pdf\n",
      "134 VishnuPrasad V PProfile.pdf\n",
      "135 NatsukiOtaniProfile.pdf\n",
      "136 Soledad De RosaProfile.pdf\n",
      "137 NURULHUDAProfile.pdf\n",
      "138 AndieGalloProfile.pdf\n",
      "139 Animator Fresher Resume.pdf\n",
      "140 AlexanderTrukhinProfile.pdf\n",
      "141 ElisaHartikkaProfile.pdf\n",
      "142 Sample Motion Graphic Designer Resume.doc\n",
      "143 3D Animator Resume.doc\n",
      "144 RafaelAlijevProfile.pdf\n",
      "145 AngelicaChieffiProfile.pdf\n",
      "146 AlexeiAltregoProfile(1).pdf\n",
      "147 DavideMarinoniProfile.pdf\n",
      "148 SaraCardelliProfile.pdf\n",
      "149 AhmedWaheedProfile.pdf\n"
     ]
    }
   ],
   "source": [
    "label_others=LoadData_others()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nilima_balkhande_Pune_1.07_yrs.doc\n",
      "2 RAHUL_KUMAR_Delhi_4.08_yrs.doc\n",
      "3 ankita_tailor_India_5.00_yrs.docx\n",
      "4 Aravind Syam Prakash.doc\n",
      "5 jasmendar_kumar_Delhi_3.00_yrs.doc\n",
      "6 Saumya__Delhi_3.00_yrs.docx\n",
      "7 Nagaraju_S_Mysore_4.05_yrs.doc\n",
      "8 GOBI.pdf\n",
      "9 Govind_Narayanan_Cochin___Kochi___Ernakulam_10.00_yrs.doc\n",
      "10 Ankit_Kumar_Bengaluru___Bangalore_1.02_yrs.docx\n",
      "11 Kiran.pdf\n",
      "12 Anitha_P_Bengaluru___Bangalore_0.00_yrs.docx\n",
      "13 naga_chaitanya_Hyderabad___Secunderabad_3.08_yrs.doc\n",
      "14 Hasan_Ali_Alavudeen_Madurai_9.00_yrs.docx\n",
      "15 Kalpesh_gurav_Thane_1.00_yrs.docx\n",
      "16 FREDIN FRANCIS.doc\n",
      "17 Raju_Jinukala_Hyderabad___Secunderabad_2.08_yrs.doc\n",
      "18 Mohd_Imran_Siddiqui_Delhi_4.00_yrs.doc\n",
      "19 mamatha_gogi_Hyderabad___Secunderabad_6.00_yrs.doc\n",
      "20 rafia_khan_Delhi_1.00_yrs.docx\n",
      "21 Harikrishnan S - Cochin _ Kochi _ Ernakulam, 3 years.pdf\n",
      "22 Anand_Shendage_Mumbai_1.06_yrs.pdf\n",
      "23 Nihal_Gesudraz_Bengaluru___Bangalore_4.02_yrs.pdf\n",
      "24 Chandrajit_Das_Hyderabad___Secunderabad_0.00_yrs.docx\n",
      "25 Neeta_Gotad_Mumbai_1.05_yrs.docx\n",
      "26 pooja_hire_Mumbai_0.00_yrs.docx\n",
      "27 Amit_Kumar__Bengaluru___Bangalore_1.00_yrs.docx\n",
      "28 sai_prasad_Hyderabad___Secunderabad_8.00_yrs.docx\n",
      "29 amir_khan_Gurgaon_2.04_yrs.doc\n",
      "30 jomini_thomas_Cochin___Kochi___Ernakulam_1.00_yrs.pdf\n",
      "31 Mohd_syed_shujayat_Hussain_Delhi_5.02_yrs.doc\n",
      "32 Nidhi_Hangal_Bengaluru___Bangalore_2.00_yrs.docx\n",
      "33 84439125_Pathanamthitta_6.05_yrs.doc\n",
      "34 Ranjeet_Kumar_Delhi_7.04_yrs.doc\n",
      "35 Manish_Saini_Jaipur_1.05_yrs.doc\n",
      "36 Radheshyam_Singh_Narwaria_Ahmedabad_3.04_yrs.docx\n",
      "37 pramod_singh_jeena_Dehradun_0.00_yrs.docx\n",
      "38 parvatham_gangadhar_Hyderabad___Secunderabad_1.00_yrs.docx\n",
      "39 RICKU.C.CHERIAN.doc\n",
      "40 Palak_Panchal_Ahmedabad_5.00_yrs.docx\n",
      "41 kapil_kumar_Delhi_10.04_yrs.pdf\n",
      "42 Samzu_K_Abu_Thrissur___Trissur_4.00_yrs.docx\n",
      "43 ANSAR  K.T.doc\n",
      "44 SANJAY_PANDEY_Gurgaon_2.00_yrs.doc\n",
      "45 Rohit K.doc\n",
      "46 Sanjeev_Kumar_Delhi_Region_6.00_yrs.docx\n",
      "47 Grandhi_Manikanta__Visakhapatnam_0.00_yrs.docx\n",
      "48 Girish_Lohar_Pune_0.00_yrs.docx\n",
      "49 prince_vaghasiya_Ahmedabad_0.00_yrs.docx\n",
      "50 Purvish_Shah__Nadiad_0.00_yrs.pdf\n",
      "51 Prashant_Chaurasia_Mumbai_3.00_yrs.docx\n",
      "52 HarshMohan[3_1].docx\n",
      "53 nikhil cv.doc\n",
      "54 Mithun_Shikari_Kolkata_5.02_yrs.docx\n",
      "55 GenafLatheef.pdf\n",
      "56 Maneesh_kumar_Mohali_4.04_yrs.docx\n",
      "57 Jitendra_vyas_Ahmedabad_6.06_yrs.doc\n",
      "58 Mohammad_Irfanuddin_Hyderabad___Secunderabad_2.03_yrs.doc\n",
      "59 Rajasekhar_Gajjela_Bengaluru___Bangalore_7.00_yrs.docx\n",
      "60 AJAY P.doc\n",
      "61 Anoopanandan.doc\n",
      "62 K_SANDEEP_REDDY_Hyderabad___Secunderabad_1.06_yrs.docx\n",
      "63 deva_raj_Bengaluru___Bangalore_2.01_yrs.docx\n",
      "64 Imteyaz_Alam_Bengaluru___Bangalore_5.00_yrs.doc\n",
      "65 Ashish_Kumar_Delhi_1.07_yrs.docx\n",
      "66 feroz_mohammad_Noida_3.01_yrs.doc\n",
      "67 Gincilin.docx\n",
      "68 Neelaveni_Murugan_Chennai_3.06_yrs.docx\n",
      "69 Hemant_Sethi_Pune_3.00_yrs.pdf\n",
      "70 Bhupinder_Singh_Chandigarh_6.00_yrs.docx\n",
      "71 EbinAntony[5_0].pdf\n",
      "72 Rakhi_Agrawal_Pune_3.00_yrs.pdf\n",
      "73 anu k raj -Interested.docx\n",
      "74 BIBIN JOSEPH.doc\n",
      "75 Nagendra_Mahamkali_Bengaluru___Bangalore_3.03_yrs.doc\n",
      "76 Ankush_Lakhorkar_Mumbai_5.00_yrs.pdf\n",
      "77 Sarath Somanath.doc\n",
      "78 santhosh_kumar_Bengaluru___Bangalore_6.06_yrs.doc\n",
      "79 45614955_Pune_9.10_yrs.docx\n",
      "80 Mehul_Bhanvadiya_Ahmedabad_2.02_yrs.pdf\n",
      "81 Hardik_Patel_Baroda_3.11_yrs.pdf\n",
      "82 Muhammed Insaf.doc\n",
      "83 Aswin.doc\n",
      "84 Mohamed_Abdelnour_Cairo_12.00_yrs.docx\n",
      "85 mahendra_verma__Lucknow_7.00_yrs.doc\n",
      "86 saravanan_govindan_Chennai_9.06_yrs.docx\n",
      "87 akhil_resume (2).doc\n",
      "88 ATT00101.docx\n",
      "89 sai_deepthi_dasari_Nellore_1.00_yrs.docx\n",
      "90 Avidhnya_Pogade_Pune_0.06_yrs.doc\n",
      "91 sandhya_seggyam_Warangal_1.09_yrs.docx\n",
      "92 Ashutosh_Upadhyay_Delhi_3.11_yrs.pdf\n",
      "93 Sabir_Ali__India_6.09_yrs.pdf\n",
      "94 Rohit_Patidar_Indore_0.00_yrs.doc\n",
      "95 Franco Abraham.pdf\n",
      "96 m_lalitha_Hyderabad___Secunderabad_2.00_yrs.docx\n",
      "97 Meenakshi__Ghaziabad_0.00_yrs.pdf\n",
      "98 Naveen_kumar_chaudhary_Delhi_2.00_yrs.docx\n",
      "99 Anil_Thakur_Gurgaon_4.03_yrs.docx\n",
      "100 Muhammed Salim.doc\n",
      "101 pankaj_PAWAR_Bhopal_3.08_yrs.docx\n",
      "102 Hari_Kishore_Naidu_Hyderabad___Secunderabad_2.02_yrs.docx\n",
      "103 Rajasekhar_Mandavelli_Hyderabad___Secunderabad_0.00_yrs.docx\n",
      "104 kallem_spandana_Hyderabad___Secunderabad_0.06_yrs.doc\n",
      "105 Rakesh Sivaraman.pdf\n",
      "106 Maithili_Joshi_Pune_3.07_yrs.doc\n",
      "107 Rafeeque.doc\n",
      "108 16561387_Delhi_3.01_yrs.doc\n",
      "109 prajakta_masane_India_3.06_yrs.doc\n",
      "110 PYARILAL.doc\n",
      "111 Abdul_Kaleem___Hyderabad___Secunderabad_3.00_yrs.doc\n",
      "112 pradip_chavan_Pune_4.06_yrs.pdf\n",
      "113 Rajeshv_Variya_Surat_0.00_yrs.pdf\n",
      "114 Muhammed Abdul Haseeb.doc\n",
      "115 Roby_Roldhan___Thrissur___Trissur_2.02_yrs.pdf\n",
      "116 HEMANT_SETHI_Pune_5.00_yrs.pdf\n",
      "117 phaneendra_dvln_Hyderabad___Secunderabad_3.06_yrs.docx\n",
      "118 Prashant_Reddy_Hyderabad___Secunderabad_2.06_yrs.doc\n",
      "119 mohd_atif_ahmad_Delhi_1.00_yrs.docx\n",
      "120 Chennaiah_Pillakadupula_Hyderabad___Secunderabad_5.02_yrs.docx\n",
      "121 Mariselvam_M_G__Coimbatore_0.04_yrs.docx\n",
      "122 Anumol_Abraham_Cochin___Kochi___Ernakulam_0.07_yrs.pdf\n",
      "123 abin.docx\n",
      "124 Gokulrah__Chennai_1.00_yrs.pdf\n",
      "125 ARUN P P.doc\n",
      "126 dhamodharan_k_Chennai_0.00_yrs.docx\n",
      "127 ratnesh_dubey_Ahmedabad_8.04_yrs.docx\n",
      "128 kamna_pandey_Delhi_0.00_yrs.pdf\n",
      "129 Alfina_Muthu_Chennai_1.05_yrs.docx\n",
      "130 saras_swathi_Chennai_1.02_yrs.docx\n",
      "131 SATHEESH.doc\n",
      "132 Priyanka_Gautam_Delhi_5.02_yrs.docx\n",
      "133 Amit_Shah_Ahmedabad_3.06_yrs.doc\n",
      "134 Akansha_Singhal_Noida_2.00_yrs.docx\n",
      "135 49700435_Bengaluru___Bangalore_16.00_yrs.doc\n",
      "136 Sanjith Rashin.doc\n",
      "137 hemalatha_sankar_Chennai_1.06_yrs.docx\n",
      "138 ARUNVISHNU R .docx\n",
      "139 Jitendra_Prajapati_Delhi_6.08_yrs.docx\n",
      "140 Azeem_Ahmad_Noida_0.06_yrs.docx\n",
      "141 Dharamendra_Singh_Delhi_5.02_yrs.doc\n",
      "142 G_Sreevallabh__Bengaluru___Bangalore_1.00_yrs.pdf\n",
      "143 Amir_Dhuri_Pune_2.08_yrs.docx\n",
      "144 anilvishnu1(sh)15thJan'16.docx\n",
      "145 Rahul_Ranjan_Delhi_12.03_yrs.docx\n",
      "146 Deepak_Chandel_Delhi_6.00_yrs.doc\n",
      "147 Rekhamudra_Sahoo_Pune_3.00_yrs.docx\n",
      "148 Basil.K.pdf\n",
      "149 Mano_Raj_Chennai_6.00_yrs.pdf\n",
      "150 Govardhan S.docx\n"
     ]
    }
   ],
   "source": [
    "label_webdesigner=LoadData_webdesigner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(label_webdesigner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jdres_train_labels=[]\n",
    "jdres_train_data=[]\n",
    "\n",
    "for row in label_admin:\n",
    "    jdres_train_data.append(row[0])\n",
    "    jdres_train_labels.append(row[1])\n",
    "#print(jdres_train_data)\n",
    "\n",
    "#print(len(jdres_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(jdres_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for row in label_webdesigner:\n",
    "    jdres_train_data.append(row[0])\n",
    "    jdres_train_labels.append(row[1])\n",
    "#print(jdres_train_labels)\n",
    "#print(jdres_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(jdres_train_data))\n",
    "print(len(jdres_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(jdres_train_data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = [0 for _ in range(len(label_admin))] + [1 for _ in range(len(label_webdesigner))]\n",
    "#print(labels)\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16270"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(jdres_train_data)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(jdres_train_data)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   12    17  3213 ...,     0     0     0]\n",
      " [   12    17  2614 ...,     0     0     0]\n",
      " [   12    17  3220 ...,     0     0     0]\n",
      " ..., \n",
      " [16211   675 16212 ...,     0     0     0]\n",
      " [  279    50  6674 ...,     0     0     0]\n",
      " [ 6679   531     8 ...,     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 500 words\n",
    "max_length = 500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = to_categorical(encoded_labels,num_classes=2)\n",
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (300, 500)\n",
      "Shape of label tensor: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(padded_docs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "padded_docs = padded_docs[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#loading glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('/home/shabna/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size,100, weights=[embedding_matrix], input_length=500, trainable='true')\n",
    "    model.add(e)\n",
    "    model.add(Conv1D(100,5, activation='relu',name='l1'))\n",
    "    model.add(MaxPooling1D(pool_size=5,name='l2'))\n",
    "    model.add(Conv1D(100, 5, activation='relu',name='l3'))\n",
    "    model.add(GlobalMaxPooling1D(name='l4'))\n",
    "    #model.add(Dense(100, activation='relu',name='l5'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(2, activation='softmax',name='l6'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          1627000   \n",
      "_________________________________________________________________\n",
      "l1 (Conv1D)                  (None, 496, 100)          50100     \n",
      "_________________________________________________________________\n",
      "l2 (MaxPooling1D)            (None, 99, 100)           0         \n",
      "_________________________________________________________________\n",
      "l3 (Conv1D)                  (None, 95, 100)           50100     \n",
      "_________________________________________________________________\n",
      "l4 (GlobalMaxPooling1D)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "l6 (Dense)                   (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,727,402\n",
      "Trainable params: 1,727,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum, batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train,validation_data=(X_val,y_val))\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"/home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=\"True\", mode=\"max\")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 5s - loss: 0.7531 - acc: 0.5600Epoch 00001: val_acc improved from -inf to 0.47500, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 14s 87ms/step - loss: 0.7134 - acc: 0.5625 - val_loss: 0.7003 - val_acc: 0.4750\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.6311 - acc: 0.6500Epoch 00002: val_acc improved from 0.47500 to 0.78750, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.5352 - acc: 0.7187 - val_loss: 0.4374 - val_acc: 0.7875\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.3556 - acc: 0.8300Epoch 00003: val_acc improved from 0.78750 to 0.93750, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.3286 - acc: 0.8750 - val_loss: 0.3153 - val_acc: 0.9375\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.2453 - acc: 0.9200Epoch 00004: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.2261 - acc: 0.9313 - val_loss: 0.3537 - val_acc: 0.8125\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.2039 - acc: 0.9000Epoch 00005: val_acc did not improve\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.1677 - acc: 0.9250 - val_loss: 0.2533 - val_acc: 0.9125\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1372 - acc: 0.9700Epoch 00006: val_acc improved from 0.93750 to 0.97500, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.1179 - acc: 0.9750 - val_loss: 0.1922 - val_acc: 0.9750\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0801 - acc: 0.9900Epoch 00007: val_acc did not improve\n",
      "160/160 [==============================] - 6s 34ms/step - loss: 0.0890 - acc: 0.9875 - val_loss: 0.1721 - val_acc: 0.9750\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0724 - acc: 1.0000Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0648 - acc: 1.0000 - val_loss: 0.1489 - val_acc: 0.9500\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0426 - acc: 1.0000Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0480 - acc: 1.0000 - val_loss: 0.1578 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0254 - acc: 1.0000Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.1503 - val_acc: 0.9375\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0341 - acc: 1.0000Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 0.9375\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0299 - acc: 1.0000Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9625\n",
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0182 - acc: 1.0000Epoch 00013: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0957 - val_acc: 0.9625\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0137 - acc: 1.0000Epoch 00014: val_acc improved from 0.97500 to 0.98750, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9875\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0203 - acc: 1.0000Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9875\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0123 - acc: 1.0000Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9625\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0146 - acc: 1.0000Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9625\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0103 - acc: 1.0000Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9625\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0061 - acc: 1.0000Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9625\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0102 - acc: 1.0000Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9625\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0056 - acc: 1.0000Epoch 00021: val_acc did not improve\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 0.9625\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0111 - acc: 1.0000Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0811 - val_acc: 0.9625\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0059 - acc: 1.0000Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9750\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0077 - acc: 1.0000Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 6s 34ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0697 - val_acc: 0.9875\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0063 - acc: 1.0000Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9875\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0053 - acc: 1.0000Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 5s 31ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0650 - val_acc: 0.9875\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0053 - acc: 1.0000Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 0.9875\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0052 - acc: 1.0000Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9875\n",
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0048 - acc: 1.0000Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9875\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0049 - acc: 1.0000Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0674 - val_acc: 0.9750\n",
      "Epoch 31/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0026 - acc: 1.0000Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0690 - val_acc: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0049 - acc: 1.0000Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0706 - val_acc: 0.9750\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0031 - acc: 1.0000Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 5s 31ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 0.9750\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0036 - acc: 1.0000Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0710 - val_acc: 0.9750\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0028 - acc: 1.0000Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0706 - val_acc: 0.9750\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0034 - acc: 1.0000Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 0.9750\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0034 - acc: 1.0000Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9750\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0043 - acc: 1.0000Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0671 - val_acc: 0.9750\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0038 - acc: 1.0000Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 0.9750\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0028 - acc: 1.0000Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9875\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0012 - acc: 1.0000Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9875\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0035 - acc: 1.0000Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9875\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0019 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 5s 31ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9875\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0044 - acc: 1.0000Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9875\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0013 - acc: 1.0000Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0679 - val_acc: 0.9750\n",
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0027 - acc: 1.0000Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0724 - val_acc: 0.9750\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0023 - acc: 1.0000Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 0.9625\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 2s - loss: 0.0015 - acc: 1.0000Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9625\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0018 - acc: 1.0000Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 0.9625\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0016 - acc: 1.0000Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0714 - val_acc: 0.9750\n",
      "2 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0394 - acc: 0.9900Epoch 00001: val_acc improved from 0.98750 to 1.00000, saving model to /home/shabna/Desktop/example_codes/weights_4_3.best.hdf5\n",
      "160/160 [==============================] - 7s 42ms/step - loss: 0.0406 - acc: 0.9875 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0224 - acc: 0.9900Epoch 00002: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0273 - acc: 0.9875 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0249 - acc: 1.0000Epoch 00003: val_acc did not improve\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0144 - acc: 1.0000Epoch 00004: val_acc did not improve\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0049 - acc: 1.0000Epoch 00005: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0074 - acc: 1.0000Epoch 00006: val_acc did not improve\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0040 - acc: 1.0000Epoch 00007: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0053 - acc: 1.0000Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0027 - acc: 1.0000Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0018 - acc: 1.0000Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0021 - acc: 1.0000Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0013 - acc: 1.0000Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0025 - acc: 1.0000Epoch 00013: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0056 - acc: 1.0000Epoch 00014: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0014 - acc: 1.0000Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 9.6820e-04 - acc: 1.0000Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 9.5454e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0010 - acc: 1.0000Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 9.1760e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0012 - acc: 1.0000Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0011 - acc: 1.0000Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 4s 27ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0010 - acc: 1.0000Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 4s 26ms/step - loss: 9.7868e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 9.1689e-04 - acc: 1.0000Epoch 00021: val_acc did not improve\n",
      "160/160 [==============================] - 4s 23ms/step - loss: 8.5212e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 7.4482e-04 - acc: 1.0000Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 7.5239e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.8099e-04 - acc: 1.0000Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 7.7697e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0010 - acc: 1.0000Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 9.1020e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 7.1853e-04 - acc: 1.0000Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 6.4312e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.6878e-04 - acc: 1.0000Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 4.7299e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.9508e-04 - acc: 1.0000Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 6.1951e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.6493e-04 - acc: 1.0000Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 6.7880e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.1290e-04 - acc: 1.0000Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 6.0411e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.5665e-04 - acc: 1.0000Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.7656e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.0106e-04 - acc: 1.0000Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 5.2916e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.8532e-04 - acc: 1.0000Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 5.1743e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 6.5199e-04 - acc: 1.0000Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 7.9311e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.9431e-04 - acc: 1.0000Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.9413e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 5.8447e-04 - acc: 1.0000Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 5.9960e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 4.4721e-04 - acc: 1.0000Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 5.3695e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.3773e-04 - acc: 1.0000Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 5.3229e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.7123e-04 - acc: 1.0000Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 5.5761e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.1830e-04 - acc: 1.0000Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 3.5525e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 6.1531e-04 - acc: 1.0000Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 5.1551e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.0053e-04 - acc: 1.0000Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 4.2636e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.5394e-04 - acc: 1.0000Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.6605e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 4.0511e-04 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 4.0554e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.4293e-04 - acc: 1.0000Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 5.2387e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.4966e-04 - acc: 1.0000Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.7502e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 3.1964e-04 - acc: 1.0000Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 4.6908e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 7.0529e-04 - acc: 1.0000Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 6.1350e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.1176e-04 - acc: 1.0000Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.3006e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.0638e-04 - acc: 1.0000Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.8483e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.8065e-04 - acc: 1.0000Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 4.0916e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "3 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.7322e-04 - acc: 1.0000Epoch 00001: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 8.7959e-04 - acc: 1.0000 - val_loss: 1.6079e-04 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.6579e-04 - acc: 1.0000Epoch 00002: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.5732e-04 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.7093e-04 - acc: 1.0000Epoch 00003: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 6.4452e-04 - acc: 1.0000 - val_loss: 1.5501e-04 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.9298e-04 - acc: 1.0000Epoch 00004: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 7.0657e-04 - acc: 1.0000 - val_loss: 1.5385e-04 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0011 - acc: 1.0000Epoch 00005: val_acc did not improve\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5550e-04 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 0.0015 - acc: 1.0000Epoch 00006: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.6358e-04 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0010 - acc: 1.0000Epoch 00007: val_acc did not improve\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7051e-04 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 7.7460e-04 - acc: 1.0000Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 6.2485e-04 - acc: 1.0000 - val_loss: 1.7821e-04 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.2230e-04 - acc: 1.0000Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 6.8817e-04 - acc: 1.0000 - val_loss: 1.8378e-04 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.1578e-04 - acc: 1.0000Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 5.2975e-04 - acc: 1.0000 - val_loss: 1.8614e-04 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.0759e-04 - acc: 1.0000Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 7.6002e-04 - acc: 1.0000 - val_loss: 1.8387e-04 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.5155e-04 - acc: 1.0000Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 4.6110e-04 - acc: 1.0000 - val_loss: 1.7689e-04 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.7679e-04 - acc: 1.0000Epoch 00013: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 4.2468e-04 - acc: 1.0000 - val_loss: 1.7019e-04 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.5137e-04 - acc: 1.0000Epoch 00014: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 6.4991e-04 - acc: 1.0000 - val_loss: 1.6453e-04 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.5581e-04 - acc: 1.0000Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 6.2993e-04 - acc: 1.0000 - val_loss: 1.5966e-04 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.4065e-04 - acc: 1.0000Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 8.9246e-04 - acc: 1.0000 - val_loss: 1.5279e-04 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.7196e-04 - acc: 1.0000Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 7.7131e-04 - acc: 1.0000 - val_loss: 1.4849e-04 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.1243e-04 - acc: 1.0000Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 4.7508e-04 - acc: 1.0000 - val_loss: 1.4536e-04 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.8141e-04 - acc: 1.0000Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 6.2225e-04 - acc: 1.0000 - val_loss: 1.4238e-04 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.2963e-04 - acc: 1.0000Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 6.3903e-04 - acc: 1.0000 - val_loss: 1.4071e-04 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.0955e-04 - acc: 1.0000Epoch 00021: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.5697e-04 - acc: 1.0000 - val_loss: 1.3960e-04 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 8.9014e-04 - acc: 1.0000Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 7.2763e-04 - acc: 1.0000 - val_loss: 1.3902e-04 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.7310e-04 - acc: 1.0000Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 5.2461e-04 - acc: 1.0000 - val_loss: 1.3825e-04 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.3513e-04 - acc: 1.0000Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.4221e-04 - acc: 1.0000 - val_loss: 1.3757e-04 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.5308e-04 - acc: 1.0000Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 5.0095e-04 - acc: 1.0000 - val_loss: 1.3744e-04 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.8588e-04 - acc: 1.0000Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.9779e-04 - acc: 1.0000 - val_loss: 1.3819e-04 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 9.0717e-04 - acc: 1.0000Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 6.7571e-04 - acc: 1.0000 - val_loss: 1.3924e-04 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 5.9466e-04 - acc: 1.0000Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 4.2878e-04 - acc: 1.0000 - val_loss: 1.3975e-04 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 6.3495e-04 - acc: 1.0000Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 5.8080e-04 - acc: 1.0000 - val_loss: 1.4098e-04 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.3703e-04 - acc: 1.0000Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.4366e-04 - acc: 1.0000 - val_loss: 1.4271e-04 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.2854e-04 - acc: 1.0000Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.4617e-04 - acc: 1.0000 - val_loss: 1.4420e-04 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.6254e-04 - acc: 1.0000Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 3.6838e-04 - acc: 1.0000 - val_loss: 1.4553e-04 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.9576e-04 - acc: 1.0000Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 4.6526e-04 - acc: 1.0000 - val_loss: 1.4601e-04 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.3798e-04 - acc: 1.0000Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.5489e-04 - acc: 1.0000 - val_loss: 1.4564e-04 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.7816e-04 - acc: 1.0000Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 4.0245e-04 - acc: 1.0000 - val_loss: 1.4500e-04 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.3947e-04 - acc: 1.0000Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.2121e-04 - acc: 1.0000 - val_loss: 1.4513e-04 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.9169e-04 - acc: 1.0000Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.7804e-04 - acc: 1.0000 - val_loss: 1.4678e-04 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.9993e-04 - acc: 1.0000Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.2643e-04 - acc: 1.0000 - val_loss: 1.4790e-04 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.1494e-04 - acc: 1.0000Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.0932e-04 - acc: 1.0000 - val_loss: 1.4833e-04 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 2.5123e-04 - acc: 1.0000Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 3.5058e-04 - acc: 1.0000 - val_loss: 1.4771e-04 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.6377e-04 - acc: 1.0000Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.2061e-04 - acc: 1.0000 - val_loss: 1.4678e-04 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.9659e-04 - acc: 1.0000Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 2.8302e-04 - acc: 1.0000 - val_loss: 1.4570e-04 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.7757e-04 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.7444e-04 - acc: 1.0000 - val_loss: 1.4619e-04 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.1188e-04 - acc: 1.0000Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.3704e-04 - acc: 1.0000 - val_loss: 1.4530e-04 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 4.5398e-04 - acc: 1.0000Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 3.4925e-04 - acc: 1.0000 - val_loss: 1.4610e-04 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.6391e-04 - acc: 1.0000Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 2.8112e-04 - acc: 1.0000 - val_loss: 1.4618e-04 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 3.6283e-04 - acc: 1.0000Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.9455e-04 - acc: 1.0000 - val_loss: 1.4455e-04 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 1.8452e-04 - acc: 1.0000Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 3.0088e-04 - acc: 1.0000 - val_loss: 1.4199e-04 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.1323e-04 - acc: 1.0000Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 1.9815e-04 - acc: 1.0000 - val_loss: 1.4039e-04 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 0s - loss: 2.7742e-04 - acc: 1.0000Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 2.7339e-04 - acc: 1.0000 - val_loss: 1.3956e-04 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "XX_train, X_test, yy_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "i=1\n",
    "for train_index, test_index in kf.split(XX_train):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(i,\":num of fold\")\n",
    "\n",
    "    X_train, X_val = XX_train[train_index], XX_train[test_index]\n",
    "    y_train, y_val = yy_train[train_index], yy_train[test_index]\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val) ,epochs=50, batch_size=100, verbose=1, callbacks=callbacks_list)\n",
    "   \n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model(\"weights_4_3.best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file = 'models4_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.065507286290327704, 0.98333334128061933]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.018944779504090549, 0.99375000000000002]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_index = {'Others':0 ,'webdesigner':1}\n",
    "target_name = [t for t in labels_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Others       1.00      0.97      0.98        29\n",
      "webdesigner       0.97      1.00      0.98        31\n",
      "\n",
      "avg / total       0.98      0.98      0.98        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(classification_report(np.argmax(y_train,axis=1),y_pred1,target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLMzMrEdOFmZm1qP/D7m+bn+MocCTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f230c2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgszMxvT/we8MWsrLH5GTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f230c2d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['page 1 ilaria perversi freelance illustratorcharacter designer summary hi everybody im ilaria nice meet you im working freelance illustrator character designer find works benetilariaperversi 2014 founded studio morsi partner angelo mozzillo motion graphic animated gifs advertisement short movies illustration bread butter create stories design characters explain company promote event… much more let’s get touch clients include vorwerk italia fondazione accenture tedx fom studio ocean butterflies international story group crearts aic yt http bitly21ozet9 wwwstudiomorsiit experience illustratore freelance gennaio 2017  present co founder marzo 2014  present indipendent studio focused animationillustration motion graphic animated gifs short movies illustration bread butter create stories design characters explain company promote event… much more education civica scuola di cinema di milano diploma digital animation 20112014 liceo linguistico gaetana agnesi diploma 20062011 page 2 honors awards jury award frammenti film festival 2015 best video project award videomakars competition best film corti vivi film fest 2014 best cartoon  comics award pistoia film festival 2014 page 3 ilaria perversi freelance illustratorcharacter designer contact ilaria linkedin http wwwlinkedincominilariaperversi5213159b']\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/Graphic Assistant.docx\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/IlariaAlquatiProfile.pdf\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/Manuele R.PennisiProfile.pdf\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/KarolinaBednorzProfile.pdf\")\n",
    "parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/others/IlariaPerversiProfile.pdf\")\n",
    "\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/Govardhan S.docx\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/nikhil cv.doc\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/Rohit K.doc\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/Rafeeque.doc\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/Others/webdesigner/PYARILAL.doc\")\n",
    "\n",
    "contents=[clean_doc(parsedPDF['content'])]\n",
    "            #print (jd_contents)\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prpInput(pp):\n",
    "    tx = Tokenizer()\n",
    "    tx.fit_on_texts(pp)\n",
    "    vocab_size = len(tx.word_index)+1\n",
    "    encoded_docs = tx.texts_to_sequences(pp)\n",
    "    max_length = 500\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  37,   1,  10,   2,  11,   6,  38,  39,  40,  12,   1,  41,\n",
       "         42,  43,  12,  44,   2,  45,  46,   6,  47,  48,  49,   3,  50,\n",
       "          7,  51,  52,  53,  54,  13,  14,  15,  16,  55,  17,  18,  19,\n",
       "         20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  56,\n",
       "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,   7,  67,  68,\n",
       "         69,  70,  71,  72,  73,  74,  32,  75,  76,  77,  78,   2,  79,\n",
       "         80,  33,  81,  82,  83,   3,  33,  84,   7,  85,  86,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  87,  88,  89,  34,  90,  34,  91,  35,  92,\n",
       "         93,  94,  95,  96,  97,  98,  35,  99,   5, 100, 101, 102, 103,\n",
       "          8, 104,   4,  36, 105,   9, 106, 107,   8, 108, 109,   9,   4,\n",
       "        110, 111,   4, 112,   3,   9, 113, 114,   8, 115,   4,  36,   3,\n",
       "          5, 116,   1,  10,   2,  11,   6, 117,   1, 118,  32, 119,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=prpInput(contents)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prd = model.predict(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Others', 1: 'webdesigner'}\n"
     ]
    }
   ],
   "source": [
    "labels_index = {'Others':0 ,'webdesigner':1}\n",
    "rev_lable_index = {}\n",
    "for key in labels_index:\n",
    "    rev_lable_index[labels_index[key]] = key\n",
    "print(rev_lable_index)\n",
    "def result(prd,contents):\n",
    "    y_classes = prd.argmax(axis=-1)\n",
    "    print(len(y_classes))\n",
    "    lx=[]\n",
    "    for idx,lb in enumerate(y_classes):\n",
    "        lx.append([contents[idx],rev_lable_index[lb]])\n",
    "    return lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['page 1 ilaria perversi freelance illustratorcharacter designer summary hi everybody im ilaria nice meet you im working freelance illustrator character designer find works benetilariaperversi 2014 founded studio morsi partner angelo mozzillo motion graphic animated gifs advertisement short movies illustration bread butter create stories design characters explain company promote event… much more let’s get touch clients include vorwerk italia fondazione accenture tedx fom studio ocean butterflies international story group crearts aic yt http bitly21ozet9 wwwstudiomorsiit experience illustratore freelance gennaio 2017  present co founder marzo 2014  present indipendent studio focused animationillustration motion graphic animated gifs short movies illustration bread butter create stories design characters explain company promote event… much more education civica scuola di cinema di milano diploma digital animation 20112014 liceo linguistico gaetana agnesi diploma 20062011 page 2 honors awards jury award frammenti film festival 2015 best video project award videomakars competition best film corti vivi film fest 2014 best cartoon  comics award pistoia film festival 2014 page 3 ilaria perversi freelance illustratorcharacter designer contact ilaria linkedin http wwwlinkedincominilariaperversi5213159b',\n",
       "  'Others']]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result(prd,contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>page 1 ilaria perversi freelance illustratorch...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        File Content   Label\n",
       "0  page 1 ilaria perversi freelance illustratorch...  Others"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result(prd,contents),columns=['File Content','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91  5]\n",
      " [11 93]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
