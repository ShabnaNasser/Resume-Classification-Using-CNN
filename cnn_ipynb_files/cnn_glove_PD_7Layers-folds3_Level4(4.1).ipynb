{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: \t\t2.0.9\n",
      "Scikit version: \t0.19.1\n",
      "TensorFlow version: \t1.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras, os, pickle, re, sklearn, string, tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adadelta, adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils  import plot_model\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "\n",
    "print('Keras version: \\t\\t%s' % keras.__version__)\n",
    "print('Scikit version: \\t%s' % sklearn.__version__)\n",
    "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tika import parser\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData_tech4():\n",
    "    label_nontech=[]\n",
    "    i=0\n",
    "    for foldername in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/software\"):\n",
    "        for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/software/\"+foldername):\n",
    "    \n",
    "            try:\n",
    "                print (i, foldername, file)\n",
    "                parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/software/\"+foldername+\"/\"+file)\n",
    "\n",
    "                resume_contents=clean_doc(parsedPDF['content'])\n",
    "            #Data = resume_contents.encode('utf-8')    \n",
    "                label_nontech.append((resume_contents,foldername))\n",
    "            except UnicodeEncodeError:\n",
    "                print ('Unicode error:', file)\n",
    "            i=i+1\n",
    "    #print (label_resume)\n",
    "    return(label_nontech)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods:\n",
    "        - Lowercase\n",
    "        - Removing whitespaces\n",
    "        - Removing stopwords\n",
    "        - Removing punctuations\n",
    "        \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    # Removing multiple whitespaces\n",
    "    doc = re.sub(r\"\\?\", \" \\? \", doc)\n",
    "    # Remove numbers\n",
    "    #doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Remove Stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    # Tokens with less then two characters will be ignored\n",
    "    #tokens = [word for word in tokens if len(word) > 1]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Other AbhishekThakurProfile.pdf\n",
      "1 Other RobertAndersonProfile.pdf\n",
      "2 Other WesScharfProfile.pdf\n",
      "3 Other HardikRathoreProfile.pdf\n",
      "4 Other VincentSProfile.pdf\n",
      "5 Other DenysKliuchProfile.pdf\n",
      "6 Other KavitaMishraProfile.pdf\n",
      "7 Other NuthanPereiraProfile.pdf\n",
      "8 Other ShantanuPrakashProfile.pdf\n",
      "9 Other MariosMichailidisProfile.pdf\n",
      "10 Other ThriveniT KProfile.pdf\n",
      "11 Other agyapalsinghProfile.pdf\n",
      "12 Other IvanRiveraProfile.pdf\n",
      "13 Other BaskaryabaseProfile.pdf\n",
      "14 Other NikitaPuzereyProfile.pdf\n",
      "15 Other Jaideep SinghBhatiProfile.pdf\n",
      "16 Other JamesJohnProfile.pdf\n",
      "17 Other RobinSharmaProfile(1).pdf\n",
      "18 Other RaghuveerSinghProfile.pdf\n",
      "19 Other VitalyKiktenkoProfile.pdf\n",
      "20 Other OleksiiBlyzniukProfile.pdf\n",
      "21 Other DeepakRamesh Profile.pdf\n",
      "22 Other GinaPooleProfile.pdf\n",
      "23 Other Dr. DeepikaSharmaProfile.pdf\n",
      "24 Other WrijuGhoshProfile.pdf\n",
      "25 Other Guruprasad GProfile.pdf\n",
      "26 Other EricMcQuigganProfile.pdf\n",
      "27 Other SerhiiYeryfaProfile.pdf\n",
      "28 Other JihinRajuProfile.pdf\n",
      "29 Other AshifIqbalProfile.pdf\n",
      "30 Other DmitriyIlchenkoProfile.pdf\n",
      "31 Other DavidGrantProfile.pdf\n",
      "32 Other OlegKolyadaProfile.pdf\n",
      "33 Other PriyaSalwanProfile.pdf\n",
      "34 Other AndreyKiselevProfile.pdf\n",
      "35 Other ShabnaMTProfile.pdf\n",
      "36 Other AbhishekKumarProfile.pdf\n",
      "37 Other StefanSchliebsProfile.pdf\n",
      "38 Other StanislavFelinskyiProfile.pdf\n",
      "39 Other RubeenaAjeedProfile.pdf\n",
      "40 Other MathewsBabuProfile.pdf\n",
      "41 Other MykhailoBoboshProfile.pdf\n",
      "42 Other SumanSauravProfile.pdf\n",
      "43 Other DougMosesProfile.pdf\n",
      "44 Other AbhinavJainProfile.pdf\n",
      "45 Other AmarnathShivashankarProfile.pdf\n",
      "46 Other DmitryDiachencoProfile.pdf\n",
      "47 Other AfanOlovcicProfile.pdf\n",
      "48 Other PoornimaSathyanarayanaProfile.pdf\n",
      "49 Other DmitryAndrushchenkoProfile.pdf\n",
      "50 Other HarishP.C. AcharyaProfile.pdf\n",
      "51 Other AndreyMedvedskiyProfile.pdf\n",
      "52 Other SrishtiGandhiProfile.pdf\n",
      "53 Other AnasJafryProfile.pdf\n",
      "54 Other Pierre-HenriRolandProfile.pdf\n",
      "55 Other AleksandrStepanenkoProfile.pdf\n",
      "56 Other AshwinThakreProfile.pdf\n",
      "57 Other ViacheslavS.Profile.pdf\n",
      "58 Other KartikKapoorProfile.pdf\n",
      "59 Other AkkiBharathProfile.pdf\n",
      "60 Other AbishekYadhvProfile.pdf\n",
      "61 Other VladyslavNalivaykoProfile.pdf\n",
      "62 Other AlexanderDubasProfile.pdf\n",
      "63 Other VivekSavsaiyaProfile.pdf\n",
      "64 Other AlexanderKonduforovProfile.pdf\n",
      "65 Other MarcRenaudProfile.pdf\n",
      "66 Other SeemaJashnaniProfile.pdf\n",
      "67 Other WillSwannackProfile.pdf\n",
      "68 Other VitaliiD.Profile.pdf\n",
      "69 Other BhavnaVermaProfile.pdf\n",
      "70 Other VladyslavNalivaykoProfile(1).pdf\n",
      "71 Other BohdanChepurnyiProfile.pdf\n",
      "72 Other AntonLebedProfile.pdf\n",
      "73 Other AnandRaghavanProfile.pdf\n",
      "74 Other NavinPrasadProfile.pdf\n",
      "75 Other RajanIyerProfile.pdf\n",
      "76 Other TylerFurnessProfile.pdf\n",
      "77 Other satishghinaiyaProfile.pdf\n",
      "78 Other ValeriiReutovProfile.pdf\n",
      "79 Other JulianTokerProfile.pdf\n",
      "80 Other AlexanderFonarevProfile.pdf\n",
      "81 Other RobinSharmaProfile.pdf\n",
      "82 Other EricWidemanProfile.pdf\n",
      "83 Other SEBASTIAN DENNYKURIAKOSEProfile.pdf\n",
      "84 Other AntonPankratovProfile.pdf\n",
      "85 Other MuhammadFarhan AqeelProfile.pdf\n",
      "86 Other AndreiKeidaProfile.pdf\n",
      "87 Other SabreenaKPProfile.pdf\n",
      "88 Other SethupalaniyappanSubramanianProfile.pdf\n",
      "89 Other VIJAYBHUVAProfile.pdf\n",
      "90 Other SauravSagarProfile.pdf\n",
      "91 Other GhaziEjazProfile.pdf\n",
      "92 Other RomaMalkovProfile.pdf\n",
      "93 Other AgatePonder-SuttonProfile.pdf\n",
      "94 Other Wade-HahnChanProfile.pdf\n",
      "95 Other MuraliTSProfile.pdf\n",
      "96 Other SumirKumarProfile.pdf\n",
      "97 Other EloyGonzalez AlpireProfile.pdf\n",
      "98 Other SergiiOstapenkoProfile.pdf\n",
      "99 Other RuslanSeleznovProfile.pdf\n",
      "100 Other PradeepParamasivamProfile.pdf\n",
      "101 Other VismayManeProfile.pdf\n",
      "102 Other AnastasiiaKornilovaProfile.pdf\n",
      "103 Other Charan DasNainiProfile.pdf\n",
      "104 Other PallaviDubeyProfile.pdf\n",
      "105 Other YuriyFilonovProfile.pdf\n",
      "106 Other MidhunsudhakarProfile.pdf\n",
      "107 Other LokeshKumarProfile.pdf\n",
      "108 Other ZeeshanAnjumProfile.pdf\n",
      "109 Other AnilDwarakanathProfile.pdf\n",
      "110 Other SaurabhSharmaProfile.pdf\n",
      "111 Other AbdulMoizProfile.pdf\n",
      "112 Other DmitrySushilovProfile.pdf\n",
      "113 Other DavidThomasProfile.pdf\n",
      "114 Other AndreyKiselevProfile(1).pdf\n",
      "115 Other YaroslavNevmerzhytskiyProfile.pdf\n",
      "116 Other AsaJohnProfile.pdf\n",
      "117 Other MaxBotvinevProfile.pdf\n",
      "118 Other AndreasWienzekProfile.pdf\n",
      "119 Other DebasisKayalProfile.pdf\n",
      "120 Other AbhishekAmbreProfile.pdf\n",
      "121 Other RishabhJainProfile.pdf\n",
      "122 Other KieranDavidsonProfile.pdf\n",
      "123 Other AbhayaAgrawalProfile.pdf\n",
      "124 Other KrishnaPriyaProfile.pdf\n",
      "125 Other VitalijMalishchukProfile.pdf\n",
      "126 Other DmitriyMorgachevProfile.pdf\n",
      "127 Other UjjwalSahayProfile.pdf\n",
      "128 Other VitaliyRadchenkoProfile.pdf\n",
      "129 Other AlexeyGrigorevProfile.pdf\n",
      "130 Other AnnaLysenkoProfile.pdf\n",
      "131 Other ScottWerbaProfile.pdf\n",
      "132 Other PavelProkopovProfile.pdf\n",
      "133 Other RamaKrishnanKProfile.pdf\n",
      "134 Other AmitKhannaProfile.pdf\n",
      "135 Other OlegIvashovProfile.pdf\n",
      "136 Other IgorBarinovProfile.pdf\n",
      "137 Other AbhishekKhandaitProfile.pdf\n",
      "138 Other CharuDixitProfile.pdf\n",
      "139 Other RiginOommenProfile.pdf\n",
      "140 Other AmanAkashProfile.pdf\n",
      "141 Other AnchalArora Profile.pdf\n",
      "142 Other NirajVishwakarmaProfile.pdf\n",
      "143 Other VladimirIglovikov, Ph.D.Profile.pdf\n",
      "144 Other _Anil KJatla _Profile.pdf\n",
      "145 Other PradeepTalikotiProfile.pdf\n",
      "146 Other VadymAkhundovProfile.pdf\n",
      "147 Other SaadAlamProfile.pdf\n",
      "148 Other NaveenKumarProfile.pdf\n",
      "149 Other AkshathkumarProfile.pdf\n",
      "150 Tester Arunayagam_Balasubramanian_United_Arab_Emirates_6.00_yrs.docx\n",
      "151 Tester VareshTuliProfile.pdf\n",
      "152 Tester ShrishtiMahoviyaProfile.pdf\n",
      "153 Tester 66432618_India_6.03_yrs.docx\n",
      "154 Tester Prashant_Koul_Gurgaon_4.04_yrs.doc\n",
      "155 Tester ArpitGuptaProfile(1).pdf\n",
      "156 Tester DILIPKUMAR__Chennai_2.00_yrs.doc\n",
      "157 Tester RakshikaSinghProfile.pdf\n",
      "158 Tester Deepa_Chandru_Chennai_7.01_yrs.doc\n",
      "159 Tester Ramya_N_Bengaluru___Bangalore_5.10_yrs.doc\n",
      "160 Tester karthik_shanker_Bengaluru___Bangalore_5.08_yrs.doc\n",
      "161 Tester Amareswara_Rao_Mumbai_5.01_yrs.docx\n",
      "162 Tester Jishachinnu_Daniel_Thane_0.09_yrs.docx\n",
      "163 Tester Jeemol_Gangadhar._Dubai_5.00_yrs.docx\n",
      "164 Tester Bharati__Bengaluru___Bangalore_2.00_yrs.doc\n",
      "165 Tester Amar_Balkawade_Navi_Mumbai_4.04_yrs.doc\n",
      "166 Tester PRASHANT M PHATATE.doc\n",
      "167 Tester nishmitha_kp_Bengaluru___Bangalore_8.05_yrs.docx\n",
      "168 Tester babul_reddy_Hyderabad___Secunderabad_3.00_yrs.docx\n",
      "169 Tester Tevin_Mathew.pdf\n",
      "170 Tester Pradeep Resume.docx\n",
      "171 Tester Rohit_Rawat_Noida_6.06_yrs.doc\n",
      "172 Tester 55714332_Pune_2.00_yrs.docx\n",
      "173 Tester deepak_agarwal_Bengaluru___Bangalore_3.07_yrs.docx\n",
      "174 Tester Karthick[4_5].pdf\n",
      "175 Tester poorva_jain_Indore_5.00_yrs.docx\n",
      "176 Tester K_R_Archanaa_Chennai_5.01_yrs.docx\n",
      "177 Tester Chethana_Surpur_Bengaluru___Bangalore_5.09_yrs.docx\n",
      "178 Tester Siddhi_Sahani_India_6.05_yrs.pdf\n",
      "179 Tester AniketShivhareProfile.pdf\n",
      "180 Tester Mohammed_Saleem_Hyderabad___Secunderabad_7.05_yrs.pdf\n",
      "181 Tester 79091303_Pune_1.09_yrs.pdf\n",
      "182 Tester SmritiSoniProfile.pdf\n",
      "183 Tester ChitraarthBhardwajProfile.pdf\n",
      "184 Tester karthik_tiyyagura_Hyderabad___Secunderabad_3.02_yrs.doc\n",
      "185 Tester jayasena_reddy_Hyderabad___Secunderabad_3.09_yrs.docx\n",
      "186 Tester Bharathi_Raja_Chennai_4.03_yrs.docx\n",
      "187 Tester samarkant_chaudhary_Noida_4.02_yrs.doc\n",
      "188 Tester sekappan_sm_Chennai_3.00_yrs.docx\n",
      "189 Tester Sanjeet_Kumar_Gurgaon_3.00_yrs.docx\n",
      "190 Tester AshishDixit.doc\n",
      "191 Tester ZishanAhmed. Profile.pdf\n",
      "192 Tester PriyankaBhatnagarProfile.pdf\n",
      "193 Tester Chirag_Dhoot_Mumbai_3.01_yrs.docx\n",
      "194 Tester 81045498_Bengaluru___Bangalore_4.03_yrs.docx\n",
      "195 Tester anurag_jain_Gurgaon_10.00_yrs.doc\n",
      "196 Tester Singh__Pune_5.00_yrs.docx\n",
      "197 Tester mahesh_singh_Noida_2.00_yrs.doc\n",
      "198 Tester dhana_lakshmi_Chennai_6.00_yrs.doc\n",
      "199 Tester Liya_Thomas_Cochin___Kochi___Ernakulam_4.02_yrs.doc\n",
      "200 Tester SunainaSutharProfile.pdf\n",
      "201 Tester RATI_KANTA_NANDA_Bhubaneshwar_5.11_yrs.doc\n",
      "202 Tester Anju_Suresh_Cochin___Kochi___Ernakulam_1.03_yrs.docx\n",
      "203 Tester Meghhaa_Tikkhe_Pune_3.00_yrs.doc\n",
      "204 Tester Namrata_Sitlani_Navi_Mumbai_3.09_yrs.pdf\n",
      "205 Tester karthickraja__Chennai_1.04_yrs.docx\n",
      "206 Tester Nitika_Chandra_Noida_5.00_yrs.pdf\n",
      "207 Tester parulagarwalProfile.pdf\n",
      "208 Tester Ramesh_Rathod_Pune_2.09_yrs.docx\n",
      "209 Tester Priyanka Mohanty_Coldfusion Java Testsing_ Atlas Systems.doc\n",
      "210 Tester Ashok_bala_Bengaluru___Bangalore_8.00_yrs.doc\n",
      "211 Tester Sathish D.pdf\n",
      "212 Tester asha_latha_Hyderabad___Secunderabad_3.03_yrs.doc\n",
      "213 Tester Rajesh Kumar M_Selenium Testing_Atlas Systems.doc\n",
      "214 Tester Praveena_Kurapati_Bengaluru___Bangalore_5.08_yrs.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 Tester HIMANSHUSHARMAProfile.pdf\n",
      "216 Tester Rohini Rattan.doc\n",
      "217 Tester BERI_KARTHIK_Hyderabad___Secunderabad_2.01_yrs.docx\n",
      "218 Tester Sarumathi_Subramani_Bengaluru___Bangalore_5.04_yrs.doc\n",
      "219 Tester Anusha A.S.doc\n",
      "220 Tester Raja_Ram_Mohan_Roy_S_Chennai_4.02_yrs.pdf\n",
      "221 Tester Anirban_Bandyopadhyay_Bengaluru___Bangalore_5.01_yrs.docx\n",
      "222 Tester Prathibha__Chilakaraju_Hyderabad___Secunderabad_7.00_yrs.doc\n",
      "223 Tester 56550931_Hyderabad___Secunderabad_5.09_yrs.doc\n",
      "224 Tester Arindam_Changdar_Bengaluru___Bangalore_0.06_yrs.docx\n",
      "225 Tester Sowjanya___Hyderabad___Secunderabad_7.00_yrs.docx\n",
      "226 Tester Harshada_Bhalerao_Pune_1.11_yrs.docx\n",
      "227 Tester Nikhil RajSinghProfile.pdf\n",
      "228 Tester Sheelu-Resume.doc\n",
      "229 Tester Murgesh_Alias_Hari_Muniyandi_Chennai_6.07_yrs.doc\n",
      "230 Tester SenthilKumar_Sundharamoorthy_Singapore_7.06_yrs.doc\n",
      "231 Tester Deepa_S_Chennai_4.06_yrs.docx\n",
      "232 Tester VINOTH KUMAR SEVAGAMURTHY.doc\n",
      "233 Tester Resume Calpine.doc\n",
      "234 Tester Saravanakumar_Jayaraman_Chennai_6.05_yrs.doc\n",
      "235 Tester Mohammadyunus__India_3.09_yrs.pdf\n",
      "236 Tester Sarath_Kumar_Chennai_4.00_yrs.docx\n",
      "237 Tester MaltiTaakProfile.pdf\n",
      "238 Tester Deepika_S_Chennai_5.00_yrs.pdf\n",
      "239 Tester ArpitGuptaProfile(2).pdf\n",
      "240 Tester Harika_Golla_Hyderabad___Secunderabad_2.00_yrs.docx\n",
      "241 Tester Manicka_Govinda_Ragul.M___Chennai_3.02_yrs.docx\n",
      "242 Tester Aamir MohammedKhanProfile.pdf\n",
      "243 Tester Shrivinothkumar_Sivakumar_Bengaluru___Bangalore_2.07_yrs.docx\n",
      "244 Tester priyanka_reddy_Hyderabad___Secunderabad_3.02_yrs.docx\n",
      "245 Tester ArghyaBiswas ( ETL - QA)Profile.pdf\n",
      "246 Tester Haritha_Madhavi_Hyderabad___Secunderabad_7.00_yrs.docx\n",
      "247 Tester Sakthi_Rajaji_Chennai_4.03_yrs.docx\n",
      "248 Tester lavanya_methuku_Bengaluru___Bangalore_2.09_yrs.docx\n",
      "249 Tester sayeed_s_k_Chennai_8.00_yrs.docx\n",
      "250 Tester Shubham_Goyal_Mumbai_4.10_yrs.docx\n",
      "251 Tester 81753864_Singapore_3.07_yrs.docx\n",
      "252 Tester 53672745_Bengaluru___Bangalore_2.05_yrs.docx\n",
      "253 Tester Saurabh_kahare_Pune_3.08_yrs.doc\n",
      "254 Tester Satya_Sundar_Barik_Bhubaneshwar_7.00_yrs.doc\n",
      "255 Tester Anjum_Hussaina_Bengaluru___Bangalore_0.00_yrs.docx\n",
      "256 Tester Sonal_Dwivedi_Mumbai_6.09_yrs.pdf\n",
      "257 Tester Harsh_Narayan_Bengaluru___Bangalore_6.00_yrs.docx\n",
      "258 Tester nilesh_nirune_Pune_11.11_yrs.docx\n",
      "259 Tester Saravanakumar_Chandrasekar_Chennai_5.10_yrs.doc\n",
      "260 Tester Naga_Gayathri_Devi_Hyderabad___Secunderabad_2.00_yrs.doc\n",
      "261 Tester savita_kharje_Mumbai_1.05_yrs.docx\n",
      "262 Tester SANDEEP_KUMAR_ELLURU__Hyderabad___Secunderabad_2.00_yrs.docx\n",
      "263 Tester Madhu_Kumar_Hyderabad___Secunderabad_2.04_yrs.docx\n",
      "264 Tester Pradeep Singh Shekhawat.doc\n",
      "265 Tester Richa Gaur.doc\n",
      "266 Tester ArghyaBiswas ( ETL - QA)Profile(1).pdf\n",
      "267 Tester mayur_jain_Pune_4.10_yrs.docx\n",
      "268 Tester Narasimharao_Dulla_Hyderabad___Secunderabad_3.06_yrs.pdf\n",
      "269 Tester JariMitroProfile(1).pdf\n",
      "270 Tester noble_mathew_Thrissur___Trissur_3.09_yrs.docx\n",
      "271 Tester Anurag_Singh_Kolkata_5.04_yrs.doc\n",
      "272 Tester RajeshV[4_5].docx\n",
      "273 Tester AnjiReddy_Automation_Chennai_Atlas Systems.doc\n",
      "274 Tester Shanmuga_Raja_Bengaluru___Bangalore_5.08_yrs.doc\n",
      "275 Tester jahnavi_prakash_macha_Mumbai_4.08_yrs.doc\n",
      "276 Tester ramakrishna_allatipalli_Hyderabad___Secunderabad_2.01_yrs.doc\n",
      "277 Tester 81951867_Mumbai_5.02_yrs.pdf\n",
      "278 Tester prem_gohil_Pune_8.00_yrs.pdf\n",
      "279 Tester Lavanya_Muthyala_Hyderabad___Secunderabad_4.06_yrs.docx\n",
      "280 Tester RiteshKumarProfile.pdf\n",
      "281 Tester PushpendraSinghProfile.pdf\n",
      "282 Tester Ramya_Rai_Bengaluru___Bangalore_0.00_yrs.docx\n",
      "283 Tester Prem_Anand__R_Chennai_0.00_yrs.docx\n",
      "284 Tester Arumuga_perumal_Chennai_3.00_yrs.doc\n",
      "285 Tester KUNAL_BENDALE_Pune_2.00_yrs.pdf\n",
      "286 Tester Shyam_Narayanan_TK_Bengaluru___Bangalore_7.01_yrs.pdf\n",
      "287 Tester Advait_Kshirsagar_Pune_4.00_yrs.docx\n",
      "288 Tester Sambasivarao__Chennai_3.10_yrs.docx\n",
      "289 Tester ArpitGuptaProfile.pdf\n",
      "290 Tester RomitaThakurProfile.pdf\n",
      "291 Tester ANUPRABHA_P_Cochin___Kochi___Ernakulam_0.08_yrs.doc\n",
      "292 Tester Pradeep_Kumar_Chennai_4.09_yrs.doc\n",
      "293 Tester PulkitGoyalProfile.pdf\n",
      "294 Tester Sravani_Madala_Hyderabad___Secunderabad_0.00_yrs.docx\n",
      "295 Tester Arun Panwar_Automation_Bangalore_Atlas Systems.doc\n",
      "296 Tester adityadhindwalProfile.pdf\n",
      "297 Tester Roopashree_Kulal_Bengaluru___Bangalore_6.06_yrs.docx\n",
      "298 Tester Arti_narode_Pune_1.07_yrs.docx\n",
      "299 Tester AMLAN_Das_Canada_6.10_yrs.doc\n",
      "300 developer Yogesh_Nikam_Pune_5.00_yrs.doc\n",
      "301 developer VanithaB[1_9].docx\n",
      "302 developer tatayyababu_kasani_Hyderabad___Secunderabad_2.02_yrs.docx\n",
      "303 developer SOURABHSHARMA[2_9].pdf\n",
      "304 developer Amol_Ramdas_Pawar___Pune_2.00_yrs.doc\n",
      "305 developer Revathy__Chennai_1.00_yrs.doc\n",
      "306 developer venkateswar_reddy_Bengaluru___Bangalore_3.02_yrs.docx\n",
      "307 developer Rajendra_B_Hyderabad___Secunderabad_11.00_yrs.doc\n",
      "308 developer jibinAhmmed[4_4].pdf\n",
      "309 developer Rama_krishna_Bengaluru___Bangalore_3.00_yrs.docx\n",
      "310 developer Naagendra_v_Hyderabad___Secunderabad_3.04_yrs.docx\n",
      "311 developer VinodPG[4_0].pdf\n",
      "312 developer Akhil Krishna VP.pdf\n",
      "313 developer NITHIN PAUL.doc\n",
      "314 developer Rishu_Kumar_Bengaluru_-_Bangalore_3.01_yrs.doc\n",
      "315 developer HarpreetSingh[1_8].doc\n",
      "316 developer Bernaditta Oshin Hogan CV.docx\n",
      "317 developer VipinGovind[3_9].pdf\n",
      "318 developer srinu[3_2].doc\n",
      "319 developer JERITT K JOSE.doc\n",
      "320 developer balaram_somineni_Hyderabad___Secunderabad_2.00_yrs.docx\n",
      "321 developer ImranMohammed - Reject.doc\n",
      "322 developer JOBIN JOSEPH.doc\n",
      "323 developer BibinBaby.pdf\n",
      "324 developer KartikiBChavan[4_5].docx\n",
      "325 developer rajat_pathak__Delhi_1.07_yrs.doc\n",
      "326 developer sneha_harihar_Bengaluru___Bangalore_2.07_yrs.pdf\n",
      "327 developer Nitheesh KS.doc\n",
      "328 developer Dhanashree_Wagal_Mumbai_4.01_yrs.docx\n",
      "329 developer sanesh_resume.doc\n",
      "330 developer KIshoreKS[6_0].doc\n",
      "331 developer Remyamol R - 6 YRS-rejected.doc\n",
      "332 developer Sreejith.S.doc\n",
      "333 developer udaya_kumar_Chennai_5.00_yrs.doc\n",
      "334 developer SAPTARSHI__Kolkata_4.00_yrs.doc\n",
      "335 developer NikhilBKuriakose[2_0].pdf\n",
      "336 developer Samresh_Kumar_Bengaluru___Bangalore_6.00_yrs.doc\n",
      "337 developer NavyaPRamesan[4_2].doc\n",
      "338 developer Shaibu rahees.doc\n",
      "339 developer SarigaResume - NI.docx\n",
      "340 developer swarnadeepan[2_0].docx\n",
      "341 developer venkatb[2_2].doc\n",
      "342 developer ramu___Mumbai_5.00_yrs.doc\n",
      "343 developer MARYSONNETXAVIER[3_1].pdf\n",
      "344 developer SubishKS.pdf\n",
      "345 developer R.Saravana Raja.doc\n",
      "346 developer Surendra_Yadav_Bhopal_9.01_yrs.doc\n",
      "347 developer 82299682_Chandigarh_0.06_yrs.docx\n",
      "348 developer DeepuStanly[1_1].docx\n",
      "349 developer Satappa_Chaugala_Bengaluru_-_Bangalore_4.05_yrs.docx\n",
      "350 developer Piyush_Dhanokar_Pune_4.00_yrs.doc\n",
      "351 developer Sivaprabha.doc\n",
      "352 developer RajaSekhar[5_5].docx\n",
      "353 developer varanambigai[2_10].docx\n",
      "354 developer Benaiah_John_Cochin___Kochi___Ernakulam_4.07_yrs.docx\n",
      "355 developer JincyWalston[2_2].doc\n",
      "356 developer venkateshwarareddy_sanaga_Bengaluru___Bangalore_2.00_yrs.doc\n",
      "357 developer VenkateshVanapalli[4_3].doc\n",
      "358 developer Biju_Resume (2).docx\n",
      "359 developer Bharathkumarreddy[4_3].doc\n",
      "360 developer SreejithS[7_0].pdf\n",
      "361 developer SivaSingh[4_2].doc\n",
      "362 developer TONYPFRANCIS[4_3].pdf\n",
      "363 developer shyamkumar[1_6].doc\n",
      "364 developer RANI JOHN- reject.doc\n",
      "365 developer SumitKumar[5_0].docx\n",
      "366 developer sushmita_Chakraborty_Bengaluru___Bangalore_13.00_yrs.doc\n",
      "367 developer pardhasaradhi[4_4].docx\n",
      "368 developer Tejas_Khele_Pune_3.07_yrs.docx\n",
      "369 developer SREEJAJS[5_5].doc\n",
      "370 developer Madhav_jain_Delhi_4.00_yrs.doc\n",
      "371 developer sumathi[1_1].docx\n",
      "372 developer KATARIRAVIVARMA[1_5].doc\n",
      "373 developer Sukhithk[4_11].doc\n",
      "374 developer pranav_joseph_Cochin___Kochi___Ernakulam_3.05_yrs.docx\n",
      "375 developer vivek_singh_bhadauria_Delhi_Region_1.06_yrs.pdf\n",
      "376 developer AnuSPillai[4_1].pdf\n",
      "377 developer Krushnamma[2_9].doc\n",
      "378 developer Vipin K V_Resume_Latest - Reject.doc\n",
      "379 developer RanjithPUthaman[5_8].pdf\n",
      "380 developer Vinayak_Fanase_Pune_4.05_yrs.docx\n",
      "381 developer muraliyandapalli[2_7].doc\n",
      "382 developer Rajeesh Radhakrishnan.doc\n",
      "383 developer Rashmi_Shrivas_Hyderabad___Secunderabad_3.00_yrs.docx\n",
      "384 developer Jagdish_Nair_Cochin___Kochi___Ernakulam_4.11_yrs.docx\n",
      "385 developer Prabha_V_Thiruvananthapuram___Trivandrum_9.10_yrs.docx\n",
      "386 developer Cethal.doc\n",
      "387 developer SHINTUV[3_0].docx\n",
      "388 developer Dipin.K.doc\n",
      "389 developer SREEKALACS[3_1].docx\n",
      "390 developer JESMI A M.doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 developer Gaurav_Balyan_Noida_4.11_yrs.docx\n",
      "392 developer srinivas[2_1].docx\n",
      "393 developer Arun_Pareek_Hyderabad___Secunderabad_4.10_yrs.pdf\n",
      "394 developer SanuRaj[3_6].pdf\n",
      "395 developer Soumyaranjan[2_5].docx\n",
      "396 developer pallavi_bhole_Mumbai_1.10_yrs.doc\n",
      "397 developer VamsiMullapudi[3_2].pdf\n",
      "398 developer Sagar[7_0].docx\n",
      "399 developer ANUSHAK[3_1].pdf\n",
      "400 developer VimalKesavan[3_11].docx\n",
      "401 developer SibinA[0_5].pdf\n",
      "402 developer SVINOTHKUMAR[4_8].docx\n",
      "403 developer Nikhil__Pune_3.00_yrs.doc\n",
      "404 developer RukminiKR[4_4].pdf\n",
      "405 developer Anil_Kumar_Nomula_Hyderabad___Secunderabad_0.06_yrs.pdf\n",
      "406 developer Lekshmi Sadanandan.doc\n",
      "407 developer AjithCPurushothaman[3_4].docx\n",
      "408 developer Sri_Rao__Malaysia_3.00_yrs.doc\n",
      "409 developer Vineet_kumar_Dubey_Mumbai_4.10_yrs.pdf\n",
      "410 developer 60659278_Bengaluru___Bangalore_4.05_yrs.docx\n",
      "411 developer shaik_karimulla__Bengaluru_-_Bangalore_0.00_yrs.docx\n",
      "412 developer vijayaramreddy[2_8].doc\n",
      "413 developer RAKESH[4_0].doc\n",
      "414 developer Sreejin T V.doc\n",
      "415 developer vishal_sonekar_Pune_4.07_yrs.doc\n",
      "416 developer VidhyaRadhakrishnan[4_4].docx\n",
      "417 developer Vikram Prasanna.docx\n",
      "418 developer sreelekshmims[3_5].pdf\n",
      "419 developer sovinjose[5_0].pdf\n",
      "420 developer Riyash_D.pdf\n",
      "421 developer venkateshwarareddy[2_0].doc\n",
      "422 developer sudeepkrishnan[2_6].doc\n",
      "423 developer SrinivasuluMeenuga[2_7].docx\n",
      "424 developer Sanjid_T.pdf\n",
      "425 developer Sambasiva_Duddukuri_Bengaluru___Bangalore_4.06_yrs.docx\n",
      "426 developer Vijeeshk[6_0].pdf\n",
      "427 developer SWETHAVK[1_4].docx\n",
      "428 developer Sreenivasa_Murthy_Chennai_12.07_yrs.doc\n",
      "429 developer Renita_Albin-Resume.docx\n",
      "430 developer SSaravanan[4_0].doc\n",
      "431 developer waseem[3_1].doc\n",
      "432 developer TintuMathew.docx\n",
      "433 developer saira_begum_Bengaluru___Bangalore_2.00_yrs.doc\n",
      "434 developer Deepak_Jain_Indore_3.00_yrs.doc\n",
      "435 developer shanmuki_p_India_4.01_yrs.docx\n",
      "436 developer ANOOP V.doc\n",
      "437 developer binu babu (resume).docx\n",
      "438 developer Vivek_Sahu_Indore_2.06_yrs.pdf\n",
      "439 developer srikanth_sriram_Hyderabad___Secunderabad_3.00_yrs.doc\n",
      "440 developer NitinJain[2_5].docx\n",
      "441 developer AlexBabu[2_3].pdf\n",
      "442 developer sojuvarughesethomas[2_3].docx\n",
      "443 developer shreya_choudhary_Bengaluru___Bangalore_2.00_yrs.doc\n",
      "444 developer Saran_kumar_Bengaluru_-_Bangalore_1.00_yrs (2).doc\n",
      "445 developer VelmuruganM[1_6].pdf\n",
      "446 developer sureshkumarreddyMV[3_9].doc\n",
      "447 developer TevinJosephKO[4_9].pdf\n",
      "448 developer JoelThomas .pdf\n",
      "449 developer SYAMILIMOLKV[2_1].docx\n"
     ]
    }
   ],
   "source": [
    "label_tech=LoadData_tech4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jdres_train_labels=[]\n",
    "jdres_train_data=[]\n",
    "\n",
    "for row in label_tech:\n",
    "    jdres_train_data.append(row[0])\n",
    "    jdres_train_labels.append(row[1])\n",
    "#print(jdres_train_data)\n",
    "#print(len(jdres_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "print(len(jdres_train_data))\n",
    "print(len(jdres_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(jdres_train_data,columns=['filecontent'])\n",
    "df2 = pd.DataFrame(jdres_train_labels,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(jdres_train_data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(jdres_train_labels)\n",
    "encoded_labels = encoder.transform(jdres_train_labels)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "labels = to_categorical(encoded_labels,num_classes=3)\n",
    "print(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(jdres_train_data)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(jdres_train_data)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   37    22  2263 ...,     0     0     0]\n",
      " [ 1017   462   490 ...,  5592   202 10579]\n",
      " [   37    22  4109 ...,     0     0     0]\n",
      " ..., \n",
      " [22912   588 22913 ...,    42  3208    90]\n",
      " [  310     5   234 ...,    40 10511   220]\n",
      " [ 7236  7237  4098 ...,     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of1000 words\n",
    "max_length = 500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (450, 500)\n",
      "Shape of label tensor: (450, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(padded_docs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "padded_docs = padded_docs[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#loading glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('/home/shabna/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size,100, weights=[embedding_matrix], input_length=500, trainable='true')\n",
    "    model.add(e)\n",
    "    model.add(Conv1D(128,5, activation='relu',name='l1'))\n",
    "    model.add(MaxPooling1D(pool_size=5,name='l2'))\n",
    "    model.add(Conv1D(128, 5, activation='relu',name='l3'))\n",
    "    model.add(GlobalMaxPooling1D(name='l4'))\n",
    "   \n",
    "    model.add(Dense(9, activation='sigmoid',name='l5'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax',name='l6'))\n",
    "    #adam = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          2300000   \n",
      "_________________________________________________________________\n",
      "l1 (Conv1D)                  (None, 496, 128)          64128     \n",
      "_________________________________________________________________\n",
      "l2 (MaxPooling1D)            (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "l3 (Conv1D)                  (None, 95, 128)           82048     \n",
      "_________________________________________________________________\n",
      "l4 (GlobalMaxPooling1D)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "l5 (Dense)                   (None, 9)                 1161      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "l6 (Dense)                   (None, 3)                 30        \n",
      "=================================================================\n",
      "Total params: 2,447,367\n",
      "Trainable params: 2,447,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = Model(embedding_matrix, preds)\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum, batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train,validation_data=(X_val,y_val))\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"/home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=\"True\", mode=\"max\")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :num of fold\n",
      "Train on 240 samples, validate on 120 samples\n",
      "Epoch 1/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 1.1651 - acc: 0.3050Epoch 00001: val_acc improved from -inf to 0.35833, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 1.1598 - acc: 0.3125 - val_loss: 1.0657 - val_acc: 0.3583\n",
      "Epoch 2/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 1.0471 - acc: 0.4350Epoch 00002: val_acc improved from 0.35833 to 0.44167, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 1.0435 - acc: 0.4333 - val_loss: 1.0007 - val_acc: 0.4417\n",
      "Epoch 3/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.9414 - acc: 0.6500Epoch 00003: val_acc improved from 0.44167 to 0.69167, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.9268 - acc: 0.6625 - val_loss: 0.9210 - val_acc: 0.6917\n",
      "Epoch 4/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.8727 - acc: 0.6550Epoch 00004: val_acc improved from 0.69167 to 0.78333, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.8615 - acc: 0.6750 - val_loss: 0.8096 - val_acc: 0.7833\n",
      "Epoch 5/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.7574 - acc: 0.8000Epoch 00005: val_acc improved from 0.78333 to 0.80000, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.7521 - acc: 0.7958 - val_loss: 0.7665 - val_acc: 0.8000\n",
      "Epoch 6/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.6926 - acc: 0.7850Epoch 00006: val_acc improved from 0.80000 to 0.84167, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.6855 - acc: 0.7917 - val_loss: 0.6997 - val_acc: 0.8417\n",
      "Epoch 7/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.6249 - acc: 0.8700Epoch 00007: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.6304 - acc: 0.8583 - val_loss: 0.6450 - val_acc: 0.8333\n",
      "Epoch 8/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.5608 - acc: 0.8400Epoch 00008: val_acc improved from 0.84167 to 0.85833, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.5531 - acc: 0.8583 - val_loss: 0.6018 - val_acc: 0.8583\n",
      "Epoch 9/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.5211 - acc: 0.9100Epoch 00009: val_acc improved from 0.85833 to 0.92500, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.5330 - acc: 0.8875 - val_loss: 0.5730 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.4835 - acc: 0.9550Epoch 00010: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.4782 - acc: 0.9500 - val_loss: 0.5261 - val_acc: 0.8917\n",
      "Epoch 11/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.4461 - acc: 0.9400Epoch 00011: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.4323 - acc: 0.9375 - val_loss: 0.4960 - val_acc: 0.9167\n",
      "Epoch 12/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.4114 - acc: 0.9500Epoch 00012: val_acc improved from 0.92500 to 0.92500, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.4134 - acc: 0.9500 - val_loss: 0.4714 - val_acc: 0.9250\n",
      "Epoch 13/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3847 - acc: 0.9450Epoch 00013: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.3744 - acc: 0.9542 - val_loss: 0.4483 - val_acc: 0.9250\n",
      "Epoch 14/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3762 - acc: 0.9650Epoch 00014: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.3658 - acc: 0.9708 - val_loss: 0.4285 - val_acc: 0.9250\n",
      "Epoch 15/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3757 - acc: 0.9350Epoch 00015: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.3769 - acc: 0.9333 - val_loss: 0.4142 - val_acc: 0.9250\n",
      "Epoch 16/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3640 - acc: 0.9600Epoch 00016: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.3715 - acc: 0.9500 - val_loss: 0.4098 - val_acc: 0.9083\n",
      "Epoch 17/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3589 - acc: 0.9450Epoch 00017: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.3658 - acc: 0.9500 - val_loss: 0.3924 - val_acc: 0.9250\n",
      "Epoch 18/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3276 - acc: 0.9650Epoch 00018: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.3210 - acc: 0.9625 - val_loss: 0.3884 - val_acc: 0.9250\n",
      "Epoch 19/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3224 - acc: 0.9600Epoch 00019: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.3309 - acc: 0.9625 - val_loss: 0.3796 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3285 - acc: 0.9500Epoch 00020: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.3163 - acc: 0.9583 - val_loss: 0.3714 - val_acc: 0.9250\n",
      "Epoch 21/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.3061 - acc: 0.9550Epoch 00021: val_acc improved from 0.92500 to 0.94167, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.3106 - acc: 0.9583 - val_loss: 0.3661 - val_acc: 0.9417\n",
      "Epoch 22/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2936 - acc: 0.9700Epoch 00022: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2969 - acc: 0.9708 - val_loss: 0.3630 - val_acc: 0.9333\n",
      "Epoch 23/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2968 - acc: 0.9650Epoch 00023: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.3001 - acc: 0.9625 - val_loss: 0.3571 - val_acc: 0.9417\n",
      "Epoch 24/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2979 - acc: 0.9750Epoch 00024: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.3012 - acc: 0.9667 - val_loss: 0.3516 - val_acc: 0.9250\n",
      "Epoch 25/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2795 - acc: 0.9800Epoch 00025: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2822 - acc: 0.9750 - val_loss: 0.3469 - val_acc: 0.9333\n",
      "Epoch 26/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2793 - acc: 0.9650Epoch 00026: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2790 - acc: 0.9708 - val_loss: 0.3423 - val_acc: 0.9417\n",
      "Epoch 27/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2893 - acc: 0.9700Epoch 00027: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2843 - acc: 0.9667 - val_loss: 0.3375 - val_acc: 0.9417\n",
      "Epoch 28/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2640 - acc: 0.9900Epoch 00028: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.2584 - acc: 0.9833 - val_loss: 0.3349 - val_acc: 0.9417\n",
      "Epoch 29/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2686 - acc: 0.9650Epoch 00029: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2715 - acc: 0.9708 - val_loss: 0.3314 - val_acc: 0.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2578 - acc: 0.9850Epoch 00030: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.2543 - acc: 0.9875 - val_loss: 0.3287 - val_acc: 0.9333\n",
      "Epoch 31/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2734 - acc: 0.9750Epoch 00031: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.2679 - acc: 0.9750 - val_loss: 0.3273 - val_acc: 0.9333\n",
      "Epoch 32/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2655 - acc: 0.9700Epoch 00032: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2623 - acc: 0.9750 - val_loss: 0.3249 - val_acc: 0.9333\n",
      "Epoch 33/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2605 - acc: 0.9650Epoch 00033: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.2652 - acc: 0.9625 - val_loss: 0.3222 - val_acc: 0.9417\n",
      "Epoch 34/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2405 - acc: 0.9850Epoch 00034: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.2370 - acc: 0.9875 - val_loss: 0.3187 - val_acc: 0.9417\n",
      "Epoch 35/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2625 - acc: 0.9550Epoch 00035: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.2616 - acc: 0.9542 - val_loss: 0.3156 - val_acc: 0.9417\n",
      "Epoch 36/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.2493 - acc: 0.9850Epoch 00036: val_acc did not improve\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.2459 - acc: 0.9833 - val_loss: 0.3134 - val_acc: 0.9333\n",
      "Epoch 37/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2504 - acc: 0.9800Epoch 00037: val_acc did not improve\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.2412 - acc: 0.9792 - val_loss: 0.3115 - val_acc: 0.9333\n",
      "Epoch 38/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2404 - acc: 0.9750Epoch 00038: val_acc did not improve\n",
      "240/240 [==============================] - 8s 32ms/step - loss: 0.2326 - acc: 0.9792 - val_loss: 0.3102 - val_acc: 0.9333\n",
      "Epoch 39/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2439 - acc: 0.9800Epoch 00039: val_acc did not improve\n",
      "240/240 [==============================] - 8s 32ms/step - loss: 0.2469 - acc: 0.9792 - val_loss: 0.3096 - val_acc: 0.9417\n",
      "Epoch 40/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2451 - acc: 0.9700Epoch 00040: val_acc did not improve\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.2371 - acc: 0.9750 - val_loss: 0.3079 - val_acc: 0.9417\n",
      "Epoch 41/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2250 - acc: 0.9950Epoch 00041: val_acc did not improve\n",
      "240/240 [==============================] - 8s 33ms/step - loss: 0.2290 - acc: 0.9958 - val_loss: 0.3040 - val_acc: 0.9417\n",
      "Epoch 42/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2273 - acc: 0.9800Epoch 00042: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.2299 - acc: 0.9833 - val_loss: 0.3011 - val_acc: 0.9333\n",
      "Epoch 43/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2312 - acc: 0.9850Epoch 00043: val_acc did not improve\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2346 - acc: 0.9792 - val_loss: 0.3000 - val_acc: 0.9333\n",
      "Epoch 44/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2181 - acc: 0.9750Epoch 00044: val_acc did not improve\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.2173 - acc: 0.9750 - val_loss: 0.2995 - val_acc: 0.9417\n",
      "Epoch 45/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2357 - acc: 0.9700Epoch 00045: val_acc did not improve\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.2410 - acc: 0.9667 - val_loss: 0.2986 - val_acc: 0.9417\n",
      "Epoch 46/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2319 - acc: 0.9800Epoch 00046: val_acc did not improve\n",
      "240/240 [==============================] - 8s 34ms/step - loss: 0.2292 - acc: 0.9833 - val_loss: 0.2969 - val_acc: 0.9333\n",
      "Epoch 47/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2281 - acc: 0.9800Epoch 00047: val_acc did not improve\n",
      "240/240 [==============================] - 8s 33ms/step - loss: 0.2229 - acc: 0.9792 - val_loss: 0.2950 - val_acc: 0.9417\n",
      "Epoch 48/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2263 - acc: 0.9650Epoch 00048: val_acc did not improve\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.2330 - acc: 0.9583 - val_loss: 0.2928 - val_acc: 0.9333\n",
      "Epoch 49/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2052 - acc: 0.9800Epoch 00049: val_acc did not improve\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.2094 - acc: 0.9792 - val_loss: 0.2909 - val_acc: 0.9417\n",
      "Epoch 50/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2232 - acc: 0.9800Epoch 00050: val_acc did not improve\n",
      "240/240 [==============================] - 8s 32ms/step - loss: 0.2195 - acc: 0.9833 - val_loss: 0.2876 - val_acc: 0.9333\n",
      "2 :num of fold\n",
      "Train on 240 samples, validate on 120 samples\n",
      "Epoch 1/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.3043 - acc: 0.9300Epoch 00001: val_acc improved from 0.94167 to 1.00000, saving model to /home/shabna/Desktop/example_codes/weights_4_1.best.hdf5\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2966 - acc: 0.9333 - val_loss: 0.1691 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2846 - acc: 0.9550Epoch 00002: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2744 - acc: 0.9542 - val_loss: 0.1674 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2434 - acc: 0.9700Epoch 00003: val_acc did not improve\n",
      "240/240 [==============================] - 7s 29ms/step - loss: 0.2456 - acc: 0.9708 - val_loss: 0.1652 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2285 - acc: 0.9800Epoch 00004: val_acc did not improve\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.2231 - acc: 0.9833 - val_loss: 0.1633 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2238 - acc: 0.9650Epoch 00005: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2186 - acc: 0.9667 - val_loss: 0.1647 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2139 - acc: 0.9750Epoch 00006: val_acc did not improve\n",
      "240/240 [==============================] - 8s 31ms/step - loss: 0.2093 - acc: 0.9792 - val_loss: 0.1616 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2012 - acc: 0.9800Epoch 00007: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.1961 - acc: 0.9792 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2197 - acc: 0.9750Epoch 00008: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2211 - acc: 0.9750 - val_loss: 0.1573 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2350 - acc: 0.9450Epoch 00009: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2296 - acc: 0.9500 - val_loss: 0.1582 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2073 - acc: 0.9750Epoch 00010: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2086 - acc: 0.9750 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1888 - acc: 0.9700Epoch 00011: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.1932 - acc: 0.9667 - val_loss: 0.1547 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2099 - acc: 0.9650Epoch 00012: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2120 - acc: 0.9625 - val_loss: 0.1523 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2005 - acc: 0.9650Epoch 00013: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.1990 - acc: 0.9625 - val_loss: 0.1507 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2000 - acc: 0.9650Epoch 00014: val_acc did not improve\n",
      "240/240 [==============================] - 7s 29ms/step - loss: 0.2021 - acc: 0.9708 - val_loss: 0.1494 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.2159 - acc: 0.9400Epoch 00015: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.2182 - acc: 0.9458 - val_loss: 0.1485 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1949 - acc: 0.9850Epoch 00016: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.1875 - acc: 0.9875 - val_loss: 0.1475 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1801 - acc: 0.9750Epoch 00017: val_acc did not improve\n",
      "240/240 [==============================] - 6s 26ms/step - loss: 0.1771 - acc: 0.9792 - val_loss: 0.1466 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1801 - acc: 0.9800Epoch 00018: val_acc did not improve\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1840 - acc: 0.9750 - val_loss: 0.1463 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1816 - acc: 0.9850Epoch 00019: val_acc did not improve\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1824 - acc: 0.9875 - val_loss: 0.1460 - val_acc: 0.9917\n",
      "Epoch 20/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1730 - acc: 0.9900Epoch 00020: val_acc did not improve\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1704 - acc: 0.9917 - val_loss: 0.1447 - val_acc: 0.9917\n",
      "Epoch 21/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1788 - acc: 0.9750Epoch 00021: val_acc did not improve\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1890 - acc: 0.9792 - val_loss: 0.1426 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1712 - acc: 0.9900Epoch 00022: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1645 - acc: 0.9875 - val_loss: 0.1405 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1814 - acc: 0.9800Epoch 00023: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1882 - acc: 0.9792 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1793 - acc: 0.9900Epoch 00024: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1801 - acc: 0.9917 - val_loss: 0.1375 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1667 - acc: 0.9900Epoch 00025: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1654 - acc: 0.9917 - val_loss: 0.1366 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1726 - acc: 0.9900Epoch 00026: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1806 - acc: 0.9750 - val_loss: 0.1358 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1817 - acc: 0.9900Epoch 00027: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1803 - acc: 0.9875 - val_loss: 0.1355 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1622 - acc: 0.9850Epoch 00028: val_acc did not improve\n",
      "240/240 [==============================] - 5s 21ms/step - loss: 0.1647 - acc: 0.9875 - val_loss: 0.1347 - val_acc: 0.9917\n",
      "Epoch 29/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1645 - acc: 0.9900Epoch 00029: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1608 - acc: 0.9917 - val_loss: 0.1329 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1980 - acc: 0.9950Epoch 00030: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1981 - acc: 0.9875 - val_loss: 0.1312 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1708 - acc: 0.9900Epoch 00031: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1758 - acc: 0.9917 - val_loss: 0.1297 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1495 - acc: 0.9950Epoch 00032: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1483 - acc: 0.9958 - val_loss: 0.1283 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1647 - acc: 0.9900Epoch 00033: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1646 - acc: 0.9917 - val_loss: 0.1272 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1622 - acc: 0.9950Epoch 00034: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1568 - acc: 0.9958 - val_loss: 0.1261 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1561 - acc: 1.0000Epoch 00035: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1539 - acc: 1.0000 - val_loss: 0.1254 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1638 - acc: 0.9850Epoch 00036: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1645 - acc: 0.9833 - val_loss: 0.1252 - val_acc: 0.9917\n",
      "Epoch 37/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1543 - acc: 0.9900Epoch 00037: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1571 - acc: 0.9917 - val_loss: 0.1241 - val_acc: 0.9917\n",
      "Epoch 38/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1573 - acc: 0.9950Epoch 00038: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1532 - acc: 0.9958 - val_loss: 0.1230 - val_acc: 0.9917\n",
      "Epoch 39/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1682 - acc: 0.9850Epoch 00039: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1698 - acc: 0.9833 - val_loss: 0.1220 - val_acc: 0.9917\n",
      "Epoch 40/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1620 - acc: 0.9900Epoch 00040: val_acc did not improve\n",
      "240/240 [==============================] - 6s 25ms/step - loss: 0.1622 - acc: 0.9875 - val_loss: 0.1203 - val_acc: 0.9917\n",
      "Epoch 41/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1546 - acc: 0.9800Epoch 00041: val_acc did not improve\n",
      "240/240 [==============================] - 6s 25ms/step - loss: 0.1529 - acc: 0.9792 - val_loss: 0.1183 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1676 - acc: 0.9900Epoch 00042: val_acc did not improve\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.1702 - acc: 0.9917 - val_loss: 0.1169 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1703 - acc: 0.9800Epoch 00043: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1724 - acc: 0.9708 - val_loss: 0.1158 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1503 - acc: 0.9900Epoch 00044: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1581 - acc: 0.9917 - val_loss: 0.1151 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1562 - acc: 0.9950Epoch 00045: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1589 - acc: 0.9917 - val_loss: 0.1144 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1594 - acc: 0.9850Epoch 00046: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1546 - acc: 0.9875 - val_loss: 0.1138 - val_acc: 0.9917\n",
      "Epoch 47/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1392 - acc: 0.9950Epoch 00047: val_acc did not improve\n",
      "240/240 [==============================] - 6s 25ms/step - loss: 0.1429 - acc: 0.9958 - val_loss: 0.1133 - val_acc: 0.9917\n",
      "Epoch 48/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1703 - acc: 0.9900Epoch 00048: val_acc did not improve\n",
      "240/240 [==============================] - 6s 25ms/step - loss: 0.1694 - acc: 0.9875 - val_loss: 0.1126 - val_acc: 0.9917\n",
      "Epoch 49/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1624 - acc: 0.9800Epoch 00049: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1611 - acc: 0.9833 - val_loss: 0.1118 - val_acc: 0.9917\n",
      "Epoch 50/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1628 - acc: 0.9950Epoch 00050: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1614 - acc: 0.9958 - val_loss: 0.1106 - val_acc: 0.9917\n",
      "3 :num of fold\n",
      "Train on 240 samples, validate on 120 samples\n",
      "Epoch 1/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1599 - acc: 0.9900Epoch 00001: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1617 - acc: 0.9875 - val_loss: 0.1014 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1406 - acc: 0.9850Epoch 00002: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1338 - acc: 0.9875 - val_loss: 0.1014 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1604 - acc: 0.9850Epoch 00003: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1571 - acc: 0.9875 - val_loss: 0.0999 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1618 - acc: 0.9850Epoch 00004: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1557 - acc: 0.9875 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1438 - acc: 1.0000Epoch 00005: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1454 - acc: 1.0000 - val_loss: 0.0976 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1325 - acc: 1.0000Epoch 00006: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1352 - acc: 1.0000 - val_loss: 0.0967 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1303 - acc: 0.9900Epoch 00007: val_acc did not improve\n",
      "240/240 [==============================] - 7s 29ms/step - loss: 0.1378 - acc: 0.9875 - val_loss: 0.0959 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1476 - acc: 0.9900Epoch 00008: val_acc did not improve\n",
      "240/240 [==============================] - 7s 30ms/step - loss: 0.1441 - acc: 0.9917 - val_loss: 0.0950 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1587 - acc: 0.9900Epoch 00009: val_acc did not improve\n",
      "240/240 [==============================] - 7s 31ms/step - loss: 0.1501 - acc: 0.9917 - val_loss: 0.0941 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1493 - acc: 0.9750Epoch 00010: val_acc did not improve\n",
      "240/240 [==============================] - 7s 29ms/step - loss: 0.1511 - acc: 0.9750 - val_loss: 0.0932 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1551 - acc: 0.9800Epoch 00011: val_acc did not improve\n",
      "240/240 [==============================] - 7s 29ms/step - loss: 0.1508 - acc: 0.9833 - val_loss: 0.0924 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1280 - acc: 0.9900Epoch 00012: val_acc did not improve\n",
      "240/240 [==============================] - 7s 27ms/step - loss: 0.1316 - acc: 0.9917 - val_loss: 0.0916 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1495 - acc: 0.9900Epoch 00013: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1486 - acc: 0.9917 - val_loss: 0.0908 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1300 - acc: 0.9950Epoch 00014: val_acc did not improve\n",
      "240/240 [==============================] - 6s 24ms/step - loss: 0.1286 - acc: 0.9917 - val_loss: 0.0900 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1164 - acc: 1.0000Epoch 00015: val_acc did not improve\n",
      "240/240 [==============================] - 5s 23ms/step - loss: 0.1166 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1533 - acc: 0.9850Epoch 00016: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1443 - acc: 0.9875 - val_loss: 0.0884 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1252 - acc: 1.0000Epoch 00017: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1336 - acc: 1.0000 - val_loss: 0.0876 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1260 - acc: 1.0000Epoch 00018: val_acc did not improve\n",
      "240/240 [==============================] - 5s 22ms/step - loss: 0.1244 - acc: 1.0000 - val_loss: 0.0868 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1375 - acc: 1.0000Epoch 00019: val_acc did not improve\n",
      "240/240 [==============================] - 6s 23ms/step - loss: 0.1383 - acc: 0.9958 - val_loss: 0.0861 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "200/240 [========================>.....] - ETA: 0s - loss: 0.1275 - acc: 1.0000Epoch 00020: val_acc did not improve\n",
      "240/240 [==============================] - 8s 34ms/step - loss: 0.1278 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1540 - acc: 0.9900Epoch 00021: val_acc did not improve\n",
      "240/240 [==============================] - 9s 39ms/step - loss: 0.1489 - acc: 0.9917 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1266 - acc: 1.0000Epoch 00022: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1206 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1546 - acc: 0.9950Epoch 00023: val_acc did not improve\n",
      "240/240 [==============================] - 11s 44ms/step - loss: 0.1473 - acc: 0.9958 - val_loss: 0.0832 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1169 - acc: 0.9900Epoch 00024: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.1201 - acc: 0.9875 - val_loss: 0.0825 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1311 - acc: 0.9950Epoch 00025: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.1289 - acc: 0.9917 - val_loss: 0.0818 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1306 - acc: 0.9900Epoch 00026: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.1307 - acc: 0.9917 - val_loss: 0.0811 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1236 - acc: 0.9900Epoch 00027: val_acc did not improve\n",
      "240/240 [==============================] - 10s 43ms/step - loss: 0.1287 - acc: 0.9875 - val_loss: 0.0804 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1194 - acc: 1.0000Epoch 00028: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1157 - acc: 1.0000 - val_loss: 0.0798 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1272 - acc: 0.9900Epoch 00029: val_acc did not improve\n",
      "240/240 [==============================] - 10s 44ms/step - loss: 0.1312 - acc: 0.9917 - val_loss: 0.0792 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1352 - acc: 0.9950Epoch 00030: val_acc did not improve\n",
      "240/240 [==============================] - 11s 46ms/step - loss: 0.1369 - acc: 0.9958 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1349 - acc: 0.9800Epoch 00031: val_acc did not improve\n",
      "240/240 [==============================] - 11s 46ms/step - loss: 0.1250 - acc: 0.9833 - val_loss: 0.0779 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1320 - acc: 0.9950Epoch 00032: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1312 - acc: 0.9958 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1150 - acc: 0.9750Epoch 00033: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1156 - acc: 0.9792 - val_loss: 0.0766 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1152 - acc: 0.9800Epoch 00034: val_acc did not improve\n",
      "240/240 [==============================] - 11s 44ms/step - loss: 0.1202 - acc: 0.9792 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1334 - acc: 0.9700Epoch 00035: val_acc did not improve\n",
      "240/240 [==============================] - 10s 43ms/step - loss: 0.1333 - acc: 0.9750 - val_loss: 0.0754 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1052 - acc: 1.0000Epoch 00036: val_acc did not improve\n",
      "240/240 [==============================] - 10s 43ms/step - loss: 0.1088 - acc: 1.0000 - val_loss: 0.0748 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1190 - acc: 0.9950Epoch 00037: val_acc did not improve\n",
      "240/240 [==============================] - 10s 43ms/step - loss: 0.1251 - acc: 0.9958 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1403 - acc: 0.9950Epoch 00038: val_acc did not improve\n",
      "240/240 [==============================] - 10s 44ms/step - loss: 0.1394 - acc: 0.9958 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1180 - acc: 0.9950Epoch 00039: val_acc did not improve\n",
      "240/240 [==============================] - 10s 43ms/step - loss: 0.1225 - acc: 0.9958 - val_loss: 0.0730 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1263 - acc: 0.9950Epoch 00040: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.1239 - acc: 0.9958 - val_loss: 0.0724 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1067 - acc: 0.9950Epoch 00041: val_acc did not improve\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.1048 - acc: 0.9958 - val_loss: 0.0718 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1241 - acc: 0.9750Epoch 00042: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1278 - acc: 0.9792 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1155 - acc: 0.9900Epoch 00043: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1121 - acc: 0.9917 - val_loss: 0.0707 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1122 - acc: 0.9900Epoch 00044: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1191 - acc: 0.9917 - val_loss: 0.0701 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1232 - acc: 0.9900Epoch 00045: val_acc did not improve\n",
      "240/240 [==============================] - 11s 48ms/step - loss: 0.1242 - acc: 0.9917 - val_loss: 0.0696 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1129 - acc: 0.9950Epoch 00046: val_acc did not improve\n",
      "240/240 [==============================] - 12s 52ms/step - loss: 0.1109 - acc: 0.9958 - val_loss: 0.0690 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1170 - acc: 0.9900Epoch 00047: val_acc did not improve\n",
      "240/240 [==============================] - 11s 45ms/step - loss: 0.1216 - acc: 0.9917 - val_loss: 0.0685 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1021 - acc: 1.0000Epoch 00048: val_acc did not improve\n",
      "240/240 [==============================] - 12s 51ms/step - loss: 0.1086 - acc: 1.0000 - val_loss: 0.0679 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.1193 - acc: 0.9850Epoch 00049: val_acc did not improve\n",
      "240/240 [==============================] - 11s 48ms/step - loss: 0.1127 - acc: 0.9875 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "200/240 [========================>.....] - ETA: 1s - loss: 0.0987 - acc: 1.0000Epoch 00050: val_acc did not improve\n",
      "240/240 [==============================] - 12s 50ms/step - loss: 0.0925 - acc: 1.0000 - val_loss: 0.0669 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "XX_train, X_test, yy_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "i=1\n",
    "for train_index, test_index in kf.split(XX_train):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(i,\":num of fold\")\n",
    "\n",
    "    X_train, X_val = XX_train[train_index], XX_train[test_index]\n",
    "    y_train, y_val = yy_train[train_index], yy_train[test_index]\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val) ,epochs=50, batch_size=100, verbose=1, callbacks=callbacks_list)\n",
    "   \n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model(\"weights_4_1.best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file = 'models.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30263475179672239, 0.92222222354676986]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21760587592919667, 0.97916666666666663]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_index = {'Other':0 ,'Tester':1,'Developer':2}\n",
    "target_name = [t for t in labels_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Other       0.93      0.93      0.93        28\n",
      "     Tester       0.91      0.97      0.94        31\n",
      "  Developer       0.93      0.87      0.90        31\n",
      "\n",
      "avg / total       0.92      0.92      0.92        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1),y_pred, target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  2]\n",
      " [ 1 30  0]\n",
      " [ 1  3 27]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(0, 1)\n",
      "(1, 2)\n",
      "(2, 0)\n",
      "(2, 0)\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "for i, item in enumerate (X_test):\n",
    "    item= item.reshape(1, 500)\n",
    "    #print (item.shape)\n",
    "    classes=model.predict(item)\n",
    "    p=(argmax(classes),argmax(y_test[i]))\n",
    "    #print(p)\n",
    "    if (argmax(classes)!=argmax(y_test[i])):\n",
    "        print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 500)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 500)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  293 10060  2298 ...,     0     0     0]\n",
      " [   37    22  2653 ...,     0     0     0]\n",
      " [   37    22  6869 ...,     0     0     0]\n",
      " ..., \n",
      " [22848 10495   957 ...,     0     0     0]\n",
      " [   37    22  5876 ...,     0     0     0]\n",
      " [12043    42  4985 ...,  6000   202 12072]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indeed resume sanjid kochi kerala sanjidt5jhxindeedemailcom  919744125676 part challenging learning environment put best creative energies competitive approach henceforth contribute pioneering fields computers information technologies education mca nehru college management 2013 2015 bca ajk college arts  science 2010 2013 skills java c j2ee sql certifications java j2ee august 2015 february 2016 additional information technical skills languages c c vb java net area interest java technologies netbeans ms visual studio dream weaver scripting languages html java script database system ms sql server ms access operating systems windows family project title judiciary supporting system organization name netgenes duration 6 months environment java db server mysql role programmer academic project abstraction judiciary supporting system project online website district  subordinate courts country upgradation infrastructure supreme court high courtsit initiated provide efficient timebound citizen centric service delivery helps enhance judicial productivity qualitatively  quantitatively make justice delivery system affordable accessible cost effective transparent citizens']\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "\n",
    "parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/software/developer/Sanjid_T.pdf\")\n",
    "\n",
    "\n",
    "contents=[clean_doc(parsedPDF['content'])]\n",
    "            \n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prpInput(pp):\n",
    "    tx = Tokenizer()\n",
    "    tx.fit_on_texts(pp)\n",
    "    vocab_size = len(tx.word_index)+1\n",
    "    encoded_doc = tx.texts_to_sequences(pp)\n",
    "    max_length = 500\n",
    "    padded_docs = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat=prpInput(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prd = model.predict(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels_index = {'marketing':0 ,'hr':1, 'seo':2, 'other':3}\n",
    "'''rev_lable_index = {}\n",
    "for key in labels_index:\n",
    "    rev_lable_index[labels_index[key]] = key\n",
    "print(rev_lable_index)'''\n",
    "def result(prd,contents):\n",
    "    y_classes = prd.argmax(axis=-1)\n",
    "    print(len(y_classes))\n",
    "    lx=[]\n",
    "    rev_lable_index = {0:'Developer' , 1: 'Other', 2: 'Tester'}\n",
    "    for idx,lb in enumerate(y_classes):\n",
    "        lx.append([contents[idx],rev_lable_index[lb]])\n",
    "    return lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['indeed resume sanjid kochi kerala sanjidt5jhxindeedemailcom  919744125676 part challenging learning environment put best creative energies competitive approach henceforth contribute pioneering fields computers information technologies education mca nehru college management 2013 2015 bca ajk college arts  science 2010 2013 skills java c j2ee sql certifications java j2ee august 2015 february 2016 additional information technical skills languages c c vb java net area interest java technologies netbeans ms visual studio dream weaver scripting languages html java script database system ms sql server ms access operating systems windows family project title judiciary supporting system organization name netgenes duration 6 months environment java db server mysql role programmer academic project abstraction judiciary supporting system project online website district  subordinate courts country upgradation infrastructure supreme court high courtsit initiated provide efficient timebound citizen centric service delivery helps enhance judicial productivity qualitatively  quantitatively make justice delivery system affordable accessible cost effective transparent citizens',\n",
       "  'Other']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result(prd,contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indeed resume sanjid kochi kerala sanjidt5jhxi...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        File Content  Label\n",
       "0  indeed resume sanjid kochi kerala sanjidt5jhxi...  Other"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(result(prd,contents),columns=['File Content','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load():\n",
    "    i=0\n",
    "    test=[]\n",
    "    for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level3/testmix/\"):\n",
    "        print (i, file)\n",
    "        parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level3/testmix/\"+file)\n",
    "        content = clean_doc(parsedPDF['content'])\n",
    "        test.append(content)\n",
    "        #mat=prpInput(contents)\n",
    "        #prd = model.predict(mat)\n",
    "        #print(result(prd,contents))\n",
    "        #pd.DataFrame(result(prd,contents),columns=['File Content','Label'])\n",
    "        i=i+1\n",
    "        \n",
    "    return test\n",
    "testinput = test_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h=prpInput(testinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prdi = model.predict(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''rev_lable_index = {}\n",
    "for key in labels_index:\n",
    "    rev_lable_index[labels_index[key]] = key\n",
    "print(rev_lable_index)'''\n",
    "def result(prdi,testinput):\n",
    "    y_classes = prdi.argmax(axis=-1)\n",
    "    print(\"num of target items:\",len(y_classes))\n",
    "    lx=[]\n",
    "    rev_lable_index= {0: 'marketing', 1: 'hr', 2: 'seo', 3: 'other'}\n",
    "    for idx,lb in enumerate(y_classes):\n",
    "        lx.append([testinput[idx],rev_lable_index[lb]])\n",
    "    return lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result(prdi,testinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result(prdi,testinput),columns=['File Content','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
