{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: \t\t2.0.9\n",
      "Scikit version: \t0.19.1\n",
      "TensorFlow version: \t1.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras, os, pickle, re, sklearn, string, tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adadelta, adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils  import plot_model\n",
    "\n",
    "print('Keras version: \\t\\t%s' % keras.__version__)\n",
    "print('Scikit version: \\t%s' % sklearn.__version__)\n",
    "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tika import parser\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData_admin():\n",
    "    label_admin=[]\n",
    "    i=0\n",
    "    for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/administrator\"):\n",
    "        try:\n",
    "            print (i, file)\n",
    "            parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/administrator/\"+file)\n",
    "\n",
    "            tech_contents=clean_doc(parsedPDF['content'])\n",
    "            #Data = resume_contents.encode('utf-8')    \n",
    "            label_admin.append((tech_contents,'Administrator'))\n",
    "        except UnicodeEncodeError:\n",
    "            print ('Unicode error:', file)\n",
    "        i=i+1\n",
    "    #print (label_resume)\n",
    "    return(label_admin)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadData_others():\n",
    "    label_others=[]\n",
    "    i=1\n",
    "    for file in os.listdir(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/others\"):\n",
    "        try:\n",
    "            print (i, file)\n",
    "            parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/others/\"+file)\n",
    "        \n",
    "            nontech_contents=clean_doc(parsedPDF['content'])\n",
    "       \n",
    "            \n",
    "            label_others.append((nontech_contents, 'Others'))\n",
    "        except UnicodeEncodeError:\n",
    "            print ('Unicode error:', file)\n",
    "        i=i+1\n",
    "            \n",
    "    return(label_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods:\n",
    "        - Lowercase\n",
    "        - Removing whitespaces\n",
    "        - Removing stopwords\n",
    "        - Removing punctuations\n",
    "        \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "   \n",
    "    tokens = doc.translate({ord(c):\"\" for c in \"\\u200b\\uf020\\u2028\\xa0\\uf0e0\\uf095\\uf041\\uf0e1\\uf0b7\\xad\"})\n",
    "    tokens = tokens.translate({ord(c):\" \" for c in \"[):,·](;•●■♦▪\"})\n",
    "    tokens = tokens.translate({ord(c):\"f\" for c in \"�\"})\n",
    "    \n",
    "    # Removing multiple whitespaces\n",
    "    tokens = re.sub(r\"\\?\", \" \\? \", tokens)\n",
    "    \n",
    "    # Split in tokens\n",
    "    tokens = tokens.split()\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Akshay_Chaudhary_Pune_10.06_yrs.doc\n",
      "1 Gudlesh_Prajapati_Mumbai_2.10_yrs.doc\n",
      "2 Binoj_P_Bengaluru___Bangalore_7.09_yrs.docx\n",
      "3 BhanuPrakash S _ MSBI_Hyderabad_Atlas Systems.doc\n",
      "4 Dhiraj_Sali_Pune_11.00_yrs.doc\n",
      "5 Abhilash Rajan.doc\n",
      "6 83604512_Chennai_12.00_yrs.doc\n",
      "7 bejoy ks.pdf\n",
      "8 Deepak_Resume 3i  (1).doc\n",
      "9 Dhirendra_Kumar_Jha_Mumbai_14.03_yrs.doc\n",
      "10 dilip_sakwadia_Pune_8.11_yrs.doc\n",
      "11 Gangadhar_NG_Hyderabad___Secunderabad_5.00_yrs.docx\n",
      "12 AKHIL_SABU_Kozhikode___Calicut_4.00_yrs.pdf\n",
      "13 A.K.Murugesan_karuppusamy_Coimbatore_5.00_yrs.doc\n",
      "14 Ajai_kumar_Bengaluru___Bangalore_6.10_yrs.doc\n",
      "15 BinuJacobc .pdf\n",
      "16 Abhimannyu_Koli_Pune_2.06_yrs.docx\n",
      "17 Arunasri_Oracle apps_Bangalore_Atlas Systems.doc\n",
      "18 Ankit_Kumar_Bengaluru___Bangalore_3.07_yrs.pdf\n",
      "19 Amit_Shambharkar_Mumbai_4.08_yrs.doc\n",
      "20 Anuj_Kumar_Delhi_1.07_yrs.docx\n",
      "21 Akhil_John_Kottayam_0.00_yrs.docx\n",
      "22 ANIL JACOB THOMAS.doc\n",
      "23 Arjun.docx\n",
      "24 Anroop Reddy _SQL DBA_Pune_Atlas Systems.doc\n",
      "25 83159981_Pune_2.02_yrs.docx\n",
      "26 ANOOP.V.doc\n",
      "27 BebinDhasC[4_5].doc\n",
      "28 Anurag_Vishnoi__Bengaluru___Bangalore_2.02_yrs.doc\n",
      "29 Dhakshin_Karthic_Coimbatore_1.05_yrs.docx\n",
      "30 dinesh_maroo_Bengaluru___Bangalore_6.06_yrs.doc\n",
      "31 arulthomas__Thanjavur_12.03_yrs.doc\n",
      "32 Ashutosh_Bajpai_Indore_21.04_yrs.pdf\n",
      "33 82568909_Ahmedabad_4.04_yrs.doc\n",
      "34 Ajay Babu M_SQL DBA_Chennai_Atlas Systems.doc\n",
      "35 ARUN_VENKATESH_RAMIAH_GUNA_SEKARAN_India_3.08_yrs.docx\n",
      "36 dinesh__Chennai_3.02_yrs.docx\n",
      "37 backiyalakshmi_M_India_7.07_yrs.docx\n",
      "38 49763448_Bengaluru___Bangalore_17.00_yrs.doc\n",
      "39 CV-jithinsp..pdf\n",
      "40 Abhishek_Jain_Pune_6.00_yrs.pdf\n",
      "41 24055553_Delhi_5.06_yrs.doc\n",
      "42 15918607_Noida_9.05_yrs.doc\n",
      "43 Devendra__Bengaluru___Bangalore_10.09_yrs.doc\n",
      "44 amanulla_syed_Hyderabad___Secunderabad_0.03_yrs.docx\n",
      "45 Bhanu_Goel_Ghaziabad_2.06_yrs.docx\n",
      "46 83886787_Hyderabad___Secunderabad_3.01_yrs.pdf\n",
      "47 Chetan_Sangule_Pune_3.00_yrs.docx\n",
      "48 faiz_shaikh_Mumbai_3.06_yrs.docx\n",
      "49 79983544_Pune_1.04_yrs.pdf\n",
      "50 Basil Joseph.pdf\n",
      "51 Chenna Reddy L  _ MSBI_Hyderabad_Atlas Systems.doc\n",
      "52 Arvind_Yadav_Noida_10.00_yrs.docx\n",
      "53 Eyad CV.doc\n",
      "54 36968539_Bengaluru___Bangalore_12.00_yrs.docx\n",
      "55 Bidyadhar_Swain_Bhubaneshwar_3.00_yrs.pdf\n",
      "56 Alankrith_Kumar__Hyderabad___Secunderabad_3.00_yrs.docx\n",
      "57 Ashish_Wankhede_Pune_1.08_yrs.docx\n",
      "58 BALASANI_SAI_CHAND_Hyderabad___Secunderabad_5.05_yrs.docx\n",
      "59 Goverdhan_Gupta_Mumbai_3.06_yrs.docx\n",
      "60 devarakonda_Hari_prasad_Anywhere_4.02_yrs.doc\n",
      "61 DEEPAK_JATHAN_Mumbai_City_10.00_yrs.docx\n",
      "62 Arun_N_Bengaluru___Bangalore_0.06_yrs.docx\n",
      "63 bhanu_b_Pune_4.00_yrs.doc\n",
      "64 AJAY_RESUME (2).pdf\n",
      "65 Deepak_Pandiyan_Bengaluru___Bangalore_5.00_yrs.doc\n",
      "66 Ananthakrishnan Resume.doc\n",
      "67 deepak_manral_Gurgaon_12.03_yrs.docx\n",
      "68 Deepak. C. M.doc\n",
      "69 Anandaraj_Rajasekaran_Chennai_3.04_yrs.docx\n",
      "70 Abhijith .pdf\n",
      "71 Anish.doc\n",
      "72 Debasish_Patel_Bengaluru___Bangalore_1.05_yrs.docx\n",
      "73 haji_pasha_Hyderabad___Secunderabad_4.08_yrs.docx\n",
      "74 Abhishek_Dhandare_Pune_3.04_yrs.docx\n",
      "75 ambrish_emmidi_Chennai_1.10_yrs.pdf\n",
      "76 Chandrakanth_Reddy__Hyderabad___Secunderabad_0.00_yrs.pdf\n",
      "77 Amit_kumar_Delhi_5.00_yrs.doc\n",
      "78 Arju_Utpal__Bengaluru___Bangalore_0.00_yrs.pdf\n",
      "79 Avinesh Paul.pdf\n",
      "80 Balaji_Ram_Chennai_13.00_yrs.doc\n",
      "81 ANOOP JOY.doc\n",
      "82 25029877_Bengaluru___Bangalore_9.00_yrs.doc\n",
      "83 Anusha___Bengaluru___Bangalore_3.01_yrs.pdf\n",
      "84 62665768_Bengaluru___Bangalore_4.03_yrs.docx\n",
      "85 dinil___Kannur_1.03_yrs.docx\n",
      "86 Chandrasekhar_Talpaneni_Bengaluru___Bangalore_3.01_yrs.docx\n",
      "87 Anil Reddy.Singam_Oracle Developer_Hyderabad_Atlas Systems.doc\n",
      "88 Arjunan_Samuthiram_Chennai_8.00_yrs.docx\n",
      "89 Atul_Gaikwad_Pune_14.11_yrs.pdf\n",
      "90 gopi_kannan_Chennai_3.07_yrs.docx\n",
      "91 akshdeep_banerjee_Kolkata_3.00_yrs.docx\n",
      "92 Akash_Banda_Bengaluru___Bangalore_0.00_yrs.doc\n",
      "93 Balaji.doc\n",
      "94 chandrasekhar_v_Chennai_5.00_yrs.docx\n",
      "95 Ajay_Kemble_Mumbai_0.00_yrs.doc\n",
      "96 chandana_reddy_Hyderabad___Secunderabad_3.02_yrs.doc\n",
      "97 Chiranjeevi__Hyderabad___Secunderabad_3.05_yrs.doc\n",
      "98 40905499_Pune_6.06_yrs.docx\n",
      "99 alok_trivedi_Delhi_Region_3.06_yrs.docx\n",
      "100 Bharath Kumar_Oracle Apps_Hyderabad_Atlas Systems.doc\n",
      "101 Gaurav_Goyal_Jaipur_2.07_yrs.doc\n",
      "102 52077267_Bengaluru___Bangalore_2.06_yrs.doc\n",
      "103 divakar_rodda_Bengaluru___Bangalore_3.00_yrs.docx\n",
      "104 Chetan_Kumar_Bengaluru___Bangalore_7.00_yrs.doc\n",
      "105 asha_mudaliar_Hyderabad___Secunderabad_3.00_yrs.docx\n",
      "106 devi_prasadh_rayapalli_Hyderabad___Secunderabad_8.00_yrs.doc\n",
      "107 Bharath_Kumar_Bengaluru___Bangalore_4.01_yrs.doc\n",
      "108 Bibhudutta_Baliarsingh_Bhubaneshwar_3.04_yrs.doc\n",
      "109 Glady_shobana_Bengaluru___Bangalore_1.02_yrs.docx\n",
      "110 A Mohan_Oracle Developer_Hyderabad_Atlas Systems.doc\n",
      "111 anik_nayak_Bengaluru___Bangalore_5.03_yrs.docx\n",
      "112 63473717_Bengaluru___Bangalore_3.00_yrs.docx\n",
      "113 Akumalli_Lalaiah_Noida_4.02_yrs.docx\n",
      "114 ANISH_GOPI_B_G.doc\n",
      "115 Ankit_Bhardwaj_United_Arab_Emirates_6.05_yrs.doc\n",
      "116 Akash_Tilak_Pune_0.00_yrs.docx\n",
      "117 Garudachalam_Chagantipati_India_7.08_yrs.docx\n",
      "118 Alvin Jaison.doc\n",
      "119 Abdul_Khalique_Mohammed_Umer_Singapore_11.00_yrs.doc\n",
      "120 Ganesh_Malkar_Pune_0.00_yrs.pdf\n",
      "121 AmalCP.pdf\n",
      "122 akshay__Pune_0.00_yrs.docx\n",
      "123 Abhijit_Kulkarni_Pune_11.00_yrs.docx\n",
      "124 Allan John -Interested.docx\n",
      "125 Anandita_Sharma_Bengaluru___Bangalore_4.00_yrs.doc\n",
      "126 Feeroz_Khan_Malaysia_5.01_yrs.doc\n",
      "127 Avinash_Nilkanth_Patil___Pune_0.00_yrs.docx\n",
      "128 geetha_elangovan_Bengaluru___Bangalore_3.06_yrs.doc\n",
      "129 Ajmal CV.doc\n",
      "130 Butchi_SQL DBA_Bangalore_Atlas Systems.doc\n",
      "131 Gurcharan__Hyderabad___Secunderabad_2.09_yrs.pdf\n",
      "132 Ashok_Mohanty_Bengaluru___Bangalore_2.00_yrs.docx\n",
      "133 ARJUNRAJ PANNEERSELVAM _Oracle DBA_Chennai_Atlas Systems.doc\n",
      "134 Abinash_Biswal_Mumbai_3.05_yrs.doc\n",
      "135 Charles M.docx\n",
      "136 Agnivo_Ganguly__Kolkata_0.06_yrs.docx\n",
      "137 80441171_Singapore_6.00_yrs.pdf\n",
      "138 BIBINMATHEW4_6.pdf\n",
      "139 Bhakti_Kulkarni_Bengaluru___Bangalore_17.04_yrs.docx\n",
      "140 baba_o_Anywhere_4.00_yrs.docx\n",
      "141 Eldhose_KJ.docx\n",
      "142 BALLA SANTIKIRAN_SQL DBA.doc\n",
      "143 ARAVIND.P.doc\n",
      "144 Anand_Kumar_Mumbai_4.00_yrs.doc\n",
      "145 80888692_Bengaluru___Bangalore_5.02_yrs.docx\n",
      "146 Guptha_Gunda_Mumbai_City_3.05_yrs.docx\n",
      "147 Damodar_reddy_Bengaluru___Bangalore_4.05_yrs.doc\n",
      "148 Aalhad_Vengurlekar_Mumbai_4.02_yrs.docx\n",
      "149 Ashish_Dahiya_Noida_3.08_yrs.doc\n"
     ]
    }
   ],
   "source": [
    "label_admin=LoadData_admin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Binoj_P_Bengaluru___Bangalore_7.09_yrs.docx\n",
      "2 YezdiRabadi (ISO-LCSM, CCSK)Profile.pdf\n",
      "3 Jose Resume.docx\n",
      "4 Cisco-Network-Engineer-Resume-Template-Free-Download.doc\n",
      "5 AtifRizviProfile.pdf\n",
      "6 kamal_gangwani_US_3.10_yrs.doc\n",
      "7 BabarShahProfile.pdf\n",
      "8 RAJESHCHAMANTHULAProfile.pdf\n",
      "9 FarshadBAGHERIANProfile.pdf\n",
      "10 kinshukchakladarProfile.pdf\n",
      "11 SarabjeetSinghProfile.pdf\n",
      "12 Muhammad MuneebIqbalProfile.pdf\n",
      "13 GaryPollard Jr.Profile.pdf\n",
      "14 SujilKumarProfile.pdf\n",
      "15 AdamKilgoreProfile.pdf\n",
      "16 BinuJacobc .pdf\n",
      "17 L.K.Sathyanarayanan__Tirunalveli_15.04_yrs (copy).docx\n",
      "18 waqassiddiquiProfile.pdf\n",
      "19 IjasAhammedProfile.pdf\n",
      "20 Anuj_Kumar_Delhi_1.07_yrs.docx\n",
      "21 CV-for-Network-Engineer-Free-Word-Download.doc\n",
      "22 Entry Level IT Networking Resume.docx\n",
      "23 MehdiMolaeiProfile.pdf\n",
      "24 Praveen KumarBommaProfile.pdf\n",
      "25 Prem_Kumar_Chennai_6.05_yrs.doc\n",
      "26 MichaelDavisProfile.pdf\n",
      "27 Network-Support-Engineer-Resume-Free-PDF-Download.pdf\n",
      "28 Tarun PratapsinghProfile.pdf\n",
      "29 83159981_Pune_2.02_yrs.docx\n",
      "30 Nishant_Soni_Pune_9.05_yrs.doc\n",
      "31 Mohammed AbakerAdamProfile.pdf\n",
      "32 SudinTRProfile.pdf\n",
      "33 DinkarSharmaProfile.pdf\n",
      "34 RiteshSinghProfile.pdf\n",
      "35 IT Specialist.doc\n",
      "36 AmitSuriProfile.pdf\n",
      "37 BadrAl-HazmiProfile.pdf\n",
      "38 EhsanAkhoondiProfile.pdf\n",
      "39 FarshidSabzkarProfile.pdf\n",
      "40 GajendraNagarajuProfile.pdf\n",
      "41 Free-Wireless-Network-Engineer-Resume-Word-Download.doc\n",
      "42 FranciscoParraProfile.pdf\n",
      "43 DebraHutsonProfile.pdf\n",
      "44 RyanS.Profile.pdf\n",
      "45 SiamakBonyadi RamProfile.pdf\n",
      "46 Syed M EhteshamShahidProfile.pdf\n",
      "47 HamedAhmadzadehProfile.pdf\n",
      "48 79983544_Pune_1.04_yrs.pdf\n",
      "49 mevin CV 2016.pdf\n",
      "50 AvinashGowdaProfile.pdf\n",
      "51 CCNA-Network-Engineer-Resume-Free-PDF-Download.pdf\n",
      "52 WaleedAsifProfile.pdf\n",
      "53 JesseStanfordProfile.pdf\n",
      "54 jothi_prakash_Bengaluru___Bangalore_8.05_yrs.doc\n",
      "55 IT Network Administrator Resume.pdf\n",
      "56 HaniGorishProfile.pdf\n",
      "57 KhalidMehmoodProfile.pdf\n",
      "58 shahrokhnikbakhtianProfile.pdf\n",
      "59 BahadorGheisari(B.Eng,PMP,CCNP)Profile.pdf\n",
      "60 Executive%20Network%20Administrator%20Resume%20Example%20Resume%20Templates%2003122018795420.pdf\n",
      "61 CCNP-Network-Engineer-Resume-Free-Word-Download.doc\n",
      "62 MarcelleLeeProfile.pdf\n",
      "63 AmitSinghProfile.pdf\n",
      "64 Raj kumarNayakProfile.pdf\n",
      "65 Carol RogerBProfile.pdf\n",
      "66 LeeConradProfile.pdf\n",
      "67 Karthikeyan_M_Pondicherry_3.01_yrs.docx\n",
      "68 Entry%20Level%20%26amp%3B%20Freshers%20Network%20Administrator%20Resume%20Resume%20Templates%2003122018162055.pdf\n",
      "69 Abhijith .pdf\n",
      "70 MohitVirmaniProfile.pdf\n",
      "71 rahulvermaProfile.pdf\n",
      "72 DivyaSubramanianProfile.pdf\n",
      "73 ManishKumarProfile.pdf\n",
      "74 Chandrakanth_Reddy__Hyderabad___Secunderabad_0.00_yrs.pdf\n",
      "75 IftikharKhanProfile.pdf\n",
      "76 SharabhKaushalProfile.pdf\n",
      "77 mohsenkheradmandProfile.pdf\n",
      "78 JithinBabu.pdf\n",
      "79 Cisco-Network-Engineer-Resume-Free-PDF-Template.pdf\n",
      "80 ShaikHussainProfile.pdf\n",
      "81 Dhameemul ansariMProfile.pdf\n",
      "82 SreekanthPProfile.pdf\n",
      "83 KartikChaubalProfile.pdf\n",
      "84 Professional%20Network%20Technician%20Resume%20Resume%20Templates%2003122018535358.pdf\n",
      "85 dinil___Kannur_1.03_yrs.docx\n",
      "86 Network-Security-Engineer-Resume-Free-PDF-Template.pdf\n",
      "87 MrShahzebProfile.pdf\n",
      "88 Manu_E_S_Cochin___Kochi___Ernakulam_4.04_yrs.pdf\n",
      "89 MunzirMubarakProfile.pdf\n",
      "90 SaeidAgheliProfile.pdf\n",
      "91 HimanshuSharma Profile.pdf\n",
      "92 Madan Mohan ReddyS.Profile.pdf\n",
      "93 40905499_Pune_6.06_yrs.docx\n",
      "94 KazemFallahiProfile.pdf\n",
      "95 Pradeep KumarMaddinaniProfile.pdf\n",
      "96 AshishThakurProfile.pdf\n",
      "97 RahulAvhadProfile.pdf\n",
      "98 RameshRamaniProfile.pdf\n",
      "99 CCNA-Network-Engineer-Resume-Free-PDF-Download(1).pdf\n",
      "100 IMRANNAWAZProfile.pdf\n",
      "101 KaranAroraProfile.pdf\n",
      "102 MohammedSaty,  OSCP  OSCE CREST CRT(PEN) ISO  LIProfile.pdf\n",
      "103 ColbyCarterProfile.pdf\n",
      "104 MahdiMirzaeiProfile.pdf\n",
      "105 PradeepHattiangadiProfile.pdf\n",
      "106 RaminOlfatiProfile.pdf\n",
      "107 TimothyFordProfile.pdf\n",
      "108 ObayAbadiProfile.pdf\n",
      "109 Abdul_Khalique_Mohammed_Umer_Singapore_11.00_yrs.doc\n",
      "110 Professional%20Network%20Engineer%20Resume%20Resume%20Templates%2003122018743267.pdf\n",
      "111 ThomasSamuel  JNCIE-SEC JNCIE-FW_VPNProfile.pdf\n",
      "112 Professional%20Network%20Administrator%20Resume%20Example%20Resume%20Templates%2003122018629827.pdf\n",
      "113 Allan John -Interested.docx\n",
      "114 linux-admin-resume-network-administrator-linux-administrator-resume-doc.ocr.pdf\n",
      "115 RandyToombsProfile.pdf\n",
      "116 KARTHIK__Chennai_0.00_yrs.docx\n",
      "117 FarashAhamadProfile.pdf\n",
      "118 KanishkChawlaProfile.pdf\n",
      "119 Himanshu_mohan_Delhi_0.00_yrs.doc\n",
      "120 PranjalVermaProfile.pdf\n",
      "121 Professional%20Network%20Administrator%20Resume%20Example%20Resume%20Templates%2003122018834196.pdf\n",
      "122 NikhilMathureProfile.pdf\n",
      "123 Sample-Network-Engineer-Resume-Free-Download.doc\n",
      "124 MatthewNikahdProfile.pdf\n",
      "125 NimaNassehiProfile.pdf\n",
      "126 MohammedFadlProfile.pdf\n",
      "127 Senior-Network-Engineer-Resume-Free-PDF-Downlaod.pdf\n",
      "128 Free-Entry-Level-Network-Engineer-Resume-Free-Word.doc\n",
      "129 BensonJonesProfile.pdf\n",
      "130 Information Technology Consultant.doc\n",
      "131 LOGANATHAN_K_Bengaluru___Bangalore_1.03_yrs.doc\n",
      "132 SajadMohammadiKobarProfile.pdf\n",
      "133 JeevithShettyProfile.pdf\n",
      "134 Mandar_ovarikar__Pune_0.00_yrs.docx\n",
      "135 AmirSatkinProfile.pdf\n",
      "136 Network-Engineer-Resume-Sample-Doc.doc\n",
      "137 SubinKumaranProfile.pdf\n",
      "138 MasoudZeynaliProfile.pdf\n",
      "139 MehdiTalebiProfile.pdf\n",
      "140 Executive%20Hardware%20Engineer%20Resume%20Resume%20Templates%2003122018743408.pdf\n",
      "141 Entry%20Level%20%26amp%3B%20Freshers%20Network%20Administrator%20Resume%20Resume%20Templates%2003122018683629.pdf\n",
      "142 MohitTanwarProfile.pdf\n",
      "143 Executive%20Network%20Administrator%20Resume%20Example%20Resume%20Templates%2003122018326897.pdf\n",
      "144 80888692_Bengaluru___Bangalore_5.02_yrs.docx\n",
      "145 ZaneMcCarreyProfile.pdf\n",
      "146 FurqanAkhtarProfile.pdf\n",
      "147 SrivatsaVasudevanProfile.pdf\n",
      "148 Professional%20Network%20Technician%20Resume%20Resume%20Templates%2003122018271762.pdf\n",
      "149 Harshad_Mahajan_Mumbai_2.07_yrs.doc\n",
      "150 SoheilErshadiProfile.pdf\n"
     ]
    }
   ],
   "source": [
    "label_others=LoadData_others()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(label_others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jdres_train_labels=[]\n",
    "jdres_train_data=[]\n",
    "\n",
    "for row in label_admin:\n",
    "    jdres_train_data.append(row[0])\n",
    "    jdres_train_labels.append(row[1])\n",
    "#print(jdres_train_data)\n",
    "\n",
    "#print(len(jdres_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(jdres_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for row in label_others:\n",
    "    jdres_train_data.append(row[0])\n",
    "    jdres_train_labels.append(row[1])\n",
    "#print(jdres_train_labels)\n",
    "#print(jdres_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(jdres_train_data))\n",
    "print(len(jdres_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(jdres_train_data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = [0 for _ in range(len(label_admin))] + [1 for _ in range(len(label_others))]\n",
    "#print(labels)\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16309"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(jdres_train_data)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(jdres_train_data)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3670 1731  223 ..., 4401 4399  371]\n",
      " [8435 5550  229 ...,    0    0    0]\n",
      " [5563 1169 3686 ...,    0    0    0]\n",
      " ..., \n",
      " [3190 3191 1082 ...,    0    0    0]\n",
      " [8339 2901 5522 ...,    0    0    0]\n",
      " [  60   44 5526 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 50 words\n",
    "max_length = 1000\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = to_categorical(encoded_labels,num_classes=2)\n",
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (300, 1000)\n",
      "Shape of label tensor: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(padded_docs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "padded_docs = padded_docs[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#loading glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('/home/shabna/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size,100, weights=[embedding_matrix], input_length=1000, trainable='true')\n",
    "    model.add(e)\n",
    "    model.add(Conv1D(100,5, activation='relu',name='l1'))\n",
    "    model.add(MaxPooling1D(pool_size=5,name='l2'))\n",
    "    model.add(Conv1D(100, 5, activation='relu',name='l3'))\n",
    "    model.add(GlobalMaxPooling1D(name='l4'))\n",
    "    #model.add(Dense(100, activation='relu',name='l5'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(2, activation='softmax',name='l6'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         1630900   \n",
      "_________________________________________________________________\n",
      "l1 (Conv1D)                  (None, 996, 100)          50100     \n",
      "_________________________________________________________________\n",
      "l2 (MaxPooling1D)            (None, 199, 100)          0         \n",
      "_________________________________________________________________\n",
      "l3 (Conv1D)                  (None, 195, 100)          50100     \n",
      "_________________________________________________________________\n",
      "l4 (GlobalMaxPooling1D)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "l6 (Dense)                   (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,731,302\n",
      "Trainable params: 1,731,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum, batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit(X_train, y_train,validation_data=(X_val,y_val))\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"/home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=\"True\", mode=\"max\")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 2s - loss: 0.7594 - acc: 0.5500Epoch 00001: val_acc improved from -inf to 0.52500, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.8968 - acc: 0.5625 - val_loss: 0.6933 - val_acc: 0.5250\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.6345 - acc: 0.6100Epoch 00002: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.7166 - acc: 0.5750 - val_loss: 0.7726 - val_acc: 0.4875\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.6310 - acc: 0.5800Epoch 00003: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.5738 - acc: 0.6437 - val_loss: 0.7028 - val_acc: 0.5125\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.5291 - acc: 0.6500Epoch 00004: val_acc improved from 0.52500 to 0.60000, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.5128 - acc: 0.6750 - val_loss: 0.5945 - val_acc: 0.6000\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.3710 - acc: 0.8200Epoch 00005: val_acc improved from 0.60000 to 0.71250, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.3326 - acc: 0.8625 - val_loss: 0.5602 - val_acc: 0.7125\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.3109 - acc: 0.9100Epoch 00006: val_acc improved from 0.71250 to 0.72500, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.3255 - acc: 0.8813 - val_loss: 0.5424 - val_acc: 0.7250\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.2659 - acc: 0.8800Epoch 00007: val_acc improved from 0.72500 to 0.83750, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.2606 - acc: 0.9000 - val_loss: 0.4955 - val_acc: 0.8375\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1772 - acc: 0.9800Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.1795 - acc: 0.9750 - val_loss: 0.5962 - val_acc: 0.6375\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.2464 - acc: 0.9500Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.2089 - acc: 0.9562 - val_loss: 0.5166 - val_acc: 0.7875\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1403 - acc: 0.9900Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.1451 - acc: 0.9875 - val_loss: 0.4657 - val_acc: 0.8250\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1115 - acc: 0.9800Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.1280 - acc: 0.9688 - val_loss: 0.4943 - val_acc: 0.7750\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1069 - acc: 0.9900Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.1310 - acc: 0.9750 - val_loss: 0.4700 - val_acc: 0.8125\n",
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1119 - acc: 0.9800Epoch 00013: val_acc improved from 0.83750 to 0.85000, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.1183 - acc: 0.9750 - val_loss: 0.4760 - val_acc: 0.8500\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0759 - acc: 0.9900Epoch 00014: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0835 - acc: 0.9875 - val_loss: 0.5123 - val_acc: 0.8250\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0768 - acc: 0.9900Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0884 - acc: 0.9812 - val_loss: 0.5018 - val_acc: 0.8375\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0925 - acc: 0.9800Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0875 - acc: 0.9813 - val_loss: 0.4716 - val_acc: 0.8250\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0716 - acc: 0.9900Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0738 - acc: 0.9812 - val_loss: 0.4714 - val_acc: 0.8250\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0791 - acc: 0.9700Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0700 - acc: 0.9813 - val_loss: 0.4787 - val_acc: 0.8125\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0902 - acc: 0.9800Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0722 - acc: 0.9875 - val_loss: 0.4779 - val_acc: 0.8250\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0604 - acc: 0.9800Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0649 - acc: 0.9750 - val_loss: 0.4853 - val_acc: 0.8250\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0783 - acc: 0.9800Epoch 00021: val_acc improved from 0.85000 to 0.86250, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0623 - acc: 0.9813 - val_loss: 0.5104 - val_acc: 0.8625\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0457 - acc: 0.9900Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0663 - acc: 0.9688 - val_loss: 0.5338 - val_acc: 0.8250\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0662 - acc: 0.9700Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0647 - acc: 0.9750 - val_loss: 0.5145 - val_acc: 0.8625\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0663 - acc: 0.9700Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0568 - acc: 0.9750 - val_loss: 0.5010 - val_acc: 0.8250\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0612 - acc: 0.9700Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0585 - acc: 0.9750 - val_loss: 0.5031 - val_acc: 0.8250\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0427 - acc: 0.9800Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0600 - acc: 0.9750 - val_loss: 0.5065 - val_acc: 0.8250\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0631 - acc: 0.9800Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0638 - acc: 0.9813 - val_loss: 0.5120 - val_acc: 0.8250\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0725 - acc: 0.9600Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0621 - acc: 0.9687 - val_loss: 0.5403 - val_acc: 0.8625\n",
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0422 - acc: 0.9900Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0463 - acc: 0.9812 - val_loss: 0.5585 - val_acc: 0.8500\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0411 - acc: 0.9900Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0513 - acc: 0.9812 - val_loss: 0.5417 - val_acc: 0.8625\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/160 [=================>............] - ETA: 1s - loss: 0.0586 - acc: 0.9700Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0467 - acc: 0.9813 - val_loss: 0.5272 - val_acc: 0.8375\n",
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0467 - acc: 0.9700Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0402 - acc: 0.9813 - val_loss: 0.5243 - val_acc: 0.8250\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0501 - acc: 0.9800Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0445 - acc: 0.9813 - val_loss: 0.5266 - val_acc: 0.8250\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0277 - acc: 0.9900Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0425 - acc: 0.9812 - val_loss: 0.5302 - val_acc: 0.8375\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0288 - acc: 0.9900Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0416 - acc: 0.9875 - val_loss: 0.5315 - val_acc: 0.8250\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0392 - acc: 0.9800Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0425 - acc: 0.9750 - val_loss: 0.5321 - val_acc: 0.8250\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0562 - acc: 0.9500Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0494 - acc: 0.9625 - val_loss: 0.5342 - val_acc: 0.8250\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0462 - acc: 0.9700Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0325 - acc: 0.9813 - val_loss: 0.5376 - val_acc: 0.8250\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0296 - acc: 0.9900Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0398 - acc: 0.9812 - val_loss: 0.5428 - val_acc: 0.8375\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0400 - acc: 0.9900Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0318 - acc: 0.9938 - val_loss: 0.5459 - val_acc: 0.8375\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0528 - acc: 0.9600Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0424 - acc: 0.9687 - val_loss: 0.5517 - val_acc: 0.8500\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0353 - acc: 0.9800Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0328 - acc: 0.9875 - val_loss: 0.5559 - val_acc: 0.8625\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0359 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0319 - acc: 0.9938 - val_loss: 0.5599 - val_acc: 0.8625\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0234 - acc: 1.0000Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0310 - acc: 0.9875 - val_loss: 0.5618 - val_acc: 0.8625\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0230 - acc: 0.9900Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0378 - acc: 0.9750 - val_loss: 0.5550 - val_acc: 0.8375\n",
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0296 - acc: 0.9900Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0326 - acc: 0.9875 - val_loss: 0.5548 - val_acc: 0.8250\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0430 - acc: 0.9900Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0444 - acc: 0.9812 - val_loss: 0.5573 - val_acc: 0.8250\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0263 - acc: 0.9900Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0276 - acc: 0.9938 - val_loss: 0.5619 - val_acc: 0.8375\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0252 - acc: 0.9900Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0401 - acc: 0.9812 - val_loss: 0.5736 - val_acc: 0.8625\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0392 - acc: 0.9700Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0400 - acc: 0.9750 - val_loss: 0.5869 - val_acc: 0.8625\n",
      "2 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.2783 - acc: 0.9300Epoch 00001: val_acc improved from 0.86250 to 0.96250, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.3132 - acc: 0.9125 - val_loss: 0.0436 - val_acc: 0.9625\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.3054 - acc: 0.9100Epoch 00002: val_acc improved from 0.96250 to 0.97500, saving model to /home/shabna/Desktop/example_codes/weights_4_2.best.hdf5\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.2848 - acc: 0.9063 - val_loss: 0.0565 - val_acc: 0.9750\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1411 - acc: 0.9400Epoch 00003: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.2060 - acc: 0.9125 - val_loss: 0.0679 - val_acc: 0.9625\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1024 - acc: 0.9500Epoch 00004: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.1406 - acc: 0.9375 - val_loss: 0.0886 - val_acc: 0.9625\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0877 - acc: 0.9700Epoch 00005: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.1180 - acc: 0.9563 - val_loss: 0.1175 - val_acc: 0.9500\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0609 - acc: 0.9800Epoch 00006: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0720 - acc: 0.9750 - val_loss: 0.1551 - val_acc: 0.9375\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0599 - acc: 0.9900Epoch 00007: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0750 - acc: 0.9750 - val_loss: 0.2009 - val_acc: 0.9375\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0702 - acc: 0.9800Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0714 - acc: 0.9813 - val_loss: 0.2196 - val_acc: 0.9375\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0695 - acc: 0.9600Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0603 - acc: 0.9687 - val_loss: 0.2233 - val_acc: 0.9375\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0449 - acc: 0.9700Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0492 - acc: 0.9750 - val_loss: 0.2319 - val_acc: 0.9375\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0593 - acc: 0.9800Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0579 - acc: 0.9813 - val_loss: 0.2473 - val_acc: 0.9375\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0278 - acc: 1.0000Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0534 - acc: 0.9750 - val_loss: 0.2722 - val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0537 - acc: 0.9900Epoch 00013: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0516 - acc: 0.9812 - val_loss: 0.2966 - val_acc: 0.9375\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0503 - acc: 0.9800Epoch 00014: val_acc did not improve\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.0529 - acc: 0.9750 - val_loss: 0.3120 - val_acc: 0.9375\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0724 - acc: 0.9600Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0630 - acc: 0.9687 - val_loss: 0.3125 - val_acc: 0.9375\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0414 - acc: 0.9800Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0593 - acc: 0.9688 - val_loss: 0.3032 - val_acc: 0.9375\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0456 - acc: 0.9700Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0464 - acc: 0.9688 - val_loss: 0.2905 - val_acc: 0.9375\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0714 - acc: 0.9500Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0479 - acc: 0.9687 - val_loss: 0.2848 - val_acc: 0.9375\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0364 - acc: 0.9800Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0449 - acc: 0.9750 - val_loss: 0.2877 - val_acc: 0.9375\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0658 - acc: 0.9700Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0589 - acc: 0.9688 - val_loss: 0.3068 - val_acc: 0.9375\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0261 - acc: 0.9800Epoch 00021: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0500 - acc: 0.9625 - val_loss: 0.3242 - val_acc: 0.9375\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0426 - acc: 0.9700Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0631 - acc: 0.9563 - val_loss: 0.3300 - val_acc: 0.9375\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0434 - acc: 0.9800Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0496 - acc: 0.9813 - val_loss: 0.3135 - val_acc: 0.9375\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0535 - acc: 0.9500Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0425 - acc: 0.9687 - val_loss: 0.3016 - val_acc: 0.9375\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0329 - acc: 0.9800Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0343 - acc: 0.9813 - val_loss: 0.2985 - val_acc: 0.9375\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0678 - acc: 0.9600Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0692 - acc: 0.9625 - val_loss: 0.3078 - val_acc: 0.9375\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0571 - acc: 0.9700Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0465 - acc: 0.9750 - val_loss: 0.3194 - val_acc: 0.9375\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0476 - acc: 0.9700Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0342 - acc: 0.9813 - val_loss: 0.3217 - val_acc: 0.9375\n",
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0231 - acc: 0.9900Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0410 - acc: 0.9750 - val_loss: 0.3279 - val_acc: 0.9375\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0689 - acc: 0.9600Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0509 - acc: 0.9687 - val_loss: 0.3268 - val_acc: 0.9375\n",
      "Epoch 31/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0266 - acc: 0.9900Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0372 - acc: 0.9875 - val_loss: 0.3208 - val_acc: 0.9375\n",
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0481 - acc: 0.9800Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0430 - acc: 0.9813 - val_loss: 0.3203 - val_acc: 0.9375\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0427 - acc: 0.9700Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0480 - acc: 0.9688 - val_loss: 0.3180 - val_acc: 0.9375\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0353 - acc: 0.9900Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0397 - acc: 0.9875 - val_loss: 0.3112 - val_acc: 0.9375\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0401 - acc: 0.9800Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0391 - acc: 0.9750 - val_loss: 0.3067 - val_acc: 0.9375\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0505 - acc: 0.9600Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0435 - acc: 0.9687 - val_loss: 0.3090 - val_acc: 0.9375\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0328 - acc: 0.9900Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0416 - acc: 0.9812 - val_loss: 0.3179 - val_acc: 0.9375\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0372 - acc: 0.9800Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0367 - acc: 0.9813 - val_loss: 0.3236 - val_acc: 0.9375\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0179 - acc: 1.0000Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0342 - acc: 0.9875 - val_loss: 0.3302 - val_acc: 0.9375\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0515 - acc: 0.9600Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0541 - acc: 0.9625 - val_loss: 0.3298 - val_acc: 0.9375\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0542 - acc: 0.9700Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0400 - acc: 0.9750 - val_loss: 0.3259 - val_acc: 0.9375\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0303 - acc: 0.9800Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0411 - acc: 0.9688 - val_loss: 0.3223 - val_acc: 0.9375\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0107 - acc: 1.0000Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0331 - acc: 0.9875 - val_loss: 0.3231 - val_acc: 0.9375\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0448 - acc: 0.9700Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0586 - acc: 0.9688 - val_loss: 0.3184 - val_acc: 0.9375\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 2s - loss: 0.0190 - acc: 0.9900Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 7s 42ms/step - loss: 0.0480 - acc: 0.9750 - val_loss: 0.3225 - val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0495 - acc: 0.9700Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 6s 34ms/step - loss: 0.0479 - acc: 0.9688 - val_loss: 0.3321 - val_acc: 0.9375\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0662 - acc: 0.9600Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0513 - acc: 0.9687 - val_loss: 0.3334 - val_acc: 0.9375\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0515 - acc: 0.9700Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0524 - acc: 0.9750 - val_loss: 0.3284 - val_acc: 0.9375\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0543 - acc: 0.9700Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0534 - acc: 0.9688 - val_loss: 0.3158 - val_acc: 0.9375\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0511 - acc: 0.9800Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0498 - acc: 0.9813 - val_loss: 0.3186 - val_acc: 0.9375\n",
      "3 :num of fold\n",
      "Train on 160 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1462 - acc: 0.9500Epoch 00001: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.1728 - acc: 0.9562 - val_loss: 0.0563 - val_acc: 0.9750\n",
      "Epoch 2/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0405 - acc: 0.9800Epoch 00002: val_acc did not improve\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.1510 - acc: 0.9563 - val_loss: 0.0645 - val_acc: 0.9750\n",
      "Epoch 3/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.1432 - acc: 0.9500Epoch 00003: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.1419 - acc: 0.9500 - val_loss: 0.0800 - val_acc: 0.9500\n",
      "Epoch 4/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0900 - acc: 0.9800Epoch 00004: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0880 - acc: 0.9688 - val_loss: 0.1059 - val_acc: 0.9375\n",
      "Epoch 5/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0756 - acc: 0.9800Epoch 00005: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0640 - acc: 0.9813 - val_loss: 0.1430 - val_acc: 0.9250\n",
      "Epoch 6/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0522 - acc: 0.9800Epoch 00006: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0467 - acc: 0.9813 - val_loss: 0.1947 - val_acc: 0.9250\n",
      "Epoch 7/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0379 - acc: 0.9900Epoch 00007: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0417 - acc: 0.9875 - val_loss: 0.2445 - val_acc: 0.9250\n",
      "Epoch 8/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0173 - acc: 1.0000Epoch 00008: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0414 - acc: 0.9812 - val_loss: 0.2858 - val_acc: 0.9250\n",
      "Epoch 9/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0543 - acc: 0.9800Epoch 00009: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0568 - acc: 0.9750 - val_loss: 0.3062 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0523 - acc: 0.9700Epoch 00010: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0496 - acc: 0.9750 - val_loss: 0.3122 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0296 - acc: 0.9900Epoch 00011: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0324 - acc: 0.9875 - val_loss: 0.3153 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0438 - acc: 0.9800Epoch 00012: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0472 - acc: 0.9813 - val_loss: 0.3321 - val_acc: 0.9250\n",
      "Epoch 13/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0435 - acc: 0.9900Epoch 00013: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0322 - acc: 0.9938 - val_loss: 0.3504 - val_acc: 0.9250\n",
      "Epoch 14/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0195 - acc: 0.9900Epoch 00014: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0412 - acc: 0.9750 - val_loss: 0.3631 - val_acc: 0.9250\n",
      "Epoch 15/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0400 - acc: 0.9900Epoch 00015: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0381 - acc: 0.9812 - val_loss: 0.3687 - val_acc: 0.9250\n",
      "Epoch 16/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0373 - acc: 0.9800Epoch 00016: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0335 - acc: 0.9813 - val_loss: 0.3817 - val_acc: 0.9250\n",
      "Epoch 17/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0249 - acc: 0.9800Epoch 00017: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0465 - acc: 0.9750 - val_loss: 0.3855 - val_acc: 0.9250\n",
      "Epoch 18/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0764 - acc: 0.9700Epoch 00018: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0540 - acc: 0.9750 - val_loss: 0.3810 - val_acc: 0.9250\n",
      "Epoch 19/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0386 - acc: 0.9800Epoch 00019: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0423 - acc: 0.9813 - val_loss: 0.3726 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0360 - acc: 0.9900Epoch 00020: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0435 - acc: 0.9812 - val_loss: 0.3740 - val_acc: 0.9250\n",
      "Epoch 21/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0705 - acc: 0.9500Epoch 00021: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0457 - acc: 0.9687 - val_loss: 0.3775 - val_acc: 0.9250\n",
      "Epoch 22/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0453 - acc: 0.9800Epoch 00022: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0364 - acc: 0.9813 - val_loss: 0.3826 - val_acc: 0.9250\n",
      "Epoch 23/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0583 - acc: 0.9800Epoch 00023: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0429 - acc: 0.9813 - val_loss: 0.3896 - val_acc: 0.9250\n",
      "Epoch 24/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0572 - acc: 0.9600Epoch 00024: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0451 - acc: 0.9687 - val_loss: 0.3947 - val_acc: 0.9250\n",
      "Epoch 25/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0306 - acc: 0.9800Epoch 00025: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0431 - acc: 0.9750 - val_loss: 0.3962 - val_acc: 0.9250\n",
      "Epoch 26/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0599 - acc: 0.9600Epoch 00026: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0421 - acc: 0.9750 - val_loss: 0.3900 - val_acc: 0.9250\n",
      "Epoch 27/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0351 - acc: 0.9700Epoch 00027: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0320 - acc: 0.9750 - val_loss: 0.3867 - val_acc: 0.9250\n",
      "Epoch 28/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0143 - acc: 0.9900Epoch 00028: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0423 - acc: 0.9812 - val_loss: 0.3873 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0536 - acc: 0.9700Epoch 00029: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0424 - acc: 0.9750 - val_loss: 0.3984 - val_acc: 0.9250\n",
      "Epoch 30/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0375 - acc: 0.9900Epoch 00030: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0367 - acc: 0.9812 - val_loss: 0.4086 - val_acc: 0.9250\n",
      "Epoch 31/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0514 - acc: 0.9700Epoch 00031: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0406 - acc: 0.9750 - val_loss: 0.4065 - val_acc: 0.9250\n",
      "Epoch 32/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0530 - acc: 0.9700Epoch 00032: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0453 - acc: 0.9750 - val_loss: 0.4036 - val_acc: 0.9250\n",
      "Epoch 33/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0454 - acc: 0.9700Epoch 00033: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0335 - acc: 0.9813 - val_loss: 0.3974 - val_acc: 0.9250\n",
      "Epoch 34/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0275 - acc: 0.9900Epoch 00034: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0377 - acc: 0.9812 - val_loss: 0.3938 - val_acc: 0.9250\n",
      "Epoch 35/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0333 - acc: 0.9700Epoch 00035: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0456 - acc: 0.9688 - val_loss: 0.3915 - val_acc: 0.9250\n",
      "Epoch 36/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0340 - acc: 0.9900Epoch 00036: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0253 - acc: 0.9938 - val_loss: 0.3975 - val_acc: 0.9250\n",
      "Epoch 37/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0398 - acc: 0.9700Epoch 00037: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0293 - acc: 0.9813 - val_loss: 0.4050 - val_acc: 0.9250\n",
      "Epoch 38/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0062 - acc: 1.0000Epoch 00038: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0216 - acc: 0.9875 - val_loss: 0.4098 - val_acc: 0.9250\n",
      "Epoch 39/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0467 - acc: 0.9700Epoch 00039: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0325 - acc: 0.9813 - val_loss: 0.4145 - val_acc: 0.9250\n",
      "Epoch 40/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0312 - acc: 0.9800Epoch 00040: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0239 - acc: 0.9875 - val_loss: 0.4147 - val_acc: 0.9250\n",
      "Epoch 41/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0355 - acc: 0.9800Epoch 00041: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0387 - acc: 0.9750 - val_loss: 0.4093 - val_acc: 0.9250\n",
      "Epoch 42/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0270 - acc: 0.9900Epoch 00042: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0280 - acc: 0.9875 - val_loss: 0.4021 - val_acc: 0.9250\n",
      "Epoch 43/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0323 - acc: 0.9800Epoch 00043: val_acc did not improve\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0274 - acc: 0.9875 - val_loss: 0.3949 - val_acc: 0.9250\n",
      "Epoch 44/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0295 - acc: 0.9900Epoch 00044: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0374 - acc: 0.9812 - val_loss: 0.3953 - val_acc: 0.9250\n",
      "Epoch 45/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0664 - acc: 0.9700Epoch 00045: val_acc did not improve\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.0614 - acc: 0.9688 - val_loss: 0.4038 - val_acc: 0.9250\n",
      "Epoch 46/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0189 - acc: 1.0000Epoch 00046: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0267 - acc: 0.9875 - val_loss: 0.4152 - val_acc: 0.9250\n",
      "Epoch 47/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0325 - acc: 0.9700Epoch 00047: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0300 - acc: 0.9750 - val_loss: 0.4206 - val_acc: 0.9250\n",
      "Epoch 48/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0223 - acc: 0.9900Epoch 00048: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0318 - acc: 0.9812 - val_loss: 0.4150 - val_acc: 0.9250\n",
      "Epoch 49/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0312 - acc: 0.9900Epoch 00049: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0272 - acc: 0.9875 - val_loss: 0.3996 - val_acc: 0.9250\n",
      "Epoch 50/50\n",
      "100/160 [=================>............] - ETA: 1s - loss: 0.0316 - acc: 0.9800Epoch 00050: val_acc did not improve\n",
      "160/160 [==============================] - 5s 33ms/step - loss: 0.0343 - acc: 0.9813 - val_loss: 0.3922 - val_acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "XX_train, X_test, yy_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "i=1\n",
    "for train_index, test_index in kf.split(XX_train):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(i,\":num of fold\")\n",
    "\n",
    "    X_train, X_val = XX_train[train_index], XX_train[test_index]\n",
    "    y_train, y_val = yy_train[train_index], yy_train[test_index]\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val) ,epochs=50, batch_size=100, verbose=1, callbacks=callbacks_list)\n",
    "   \n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model(\"weights_4_2.best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file = 'models4_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40776973168055214, 0.89999999602635705]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23033140748739242, 0.89375000000000004]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_index = {'Administrator':0 ,'Others':1}\n",
    "target_name = [t for t in labels_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Administrator       0.85      1.00      0.92        34\n",
      "       Others       1.00      0.77      0.87        26\n",
      "\n",
      "  avg / total       0.91      0.90      0.90        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(classification_report(np.argmax(y_train,axis=1),y_pred1,target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLMzMrEdOFmZm1qP/D7m+bn+MocCTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f230c2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgszMxvT/we8MWsrLH5GTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f230c2d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['curriculum vitae anuj kumar s84a sec52 noida email canujchauhan1gmailcom mobile 91 9871084579 objective seeking position utilize skills abilities information technology industry offers professional growth resourceful innovative certification  training rhcsa  rhce certification number 140201286 certified red hat https wwwredhatcomwappstrainingcertificationverifyhtml  certnumber140201286issearchfalseverifyverify 3months training rhel 60 network nuts new delhi work experience since march 2014 indian council agriculture research icar new delhi till present linux admin roles responsibility installation configuration red hat linux 67 ubuntu nagios monitoring tool configure git shell scripting creating  configuration yum repository managing users  groups acl administration maintenance support samba sftp ftp dns ssh apache nfs server configure apachetomcat server configure squid cache server configure haproxy apache tuning configure awsec2 vpc application system services – lvm data backup nagios iptables monitoring disk space management ensuring data security installing removing software yumrpm’s change ownership permission directory backup tools like tar dump configure smtp postfix dovecot mail managing flexible storage logical volume manager lvm  analyzing storing logs  managing process file security gnupg web hosting  monitor web server change ownership permission directory database backup  restoration database tuning check daily physically equipment maintain daily health check report server’s managing red hat enterprise virtualization manager hypervisor virtualization environment configuration create managing virtual machine templates pools users host network security virtualized environment managing red hat virtualization environment monitoring reports migration high availability establishing documenting procedures ensure integrity data including system failure backup recovery monitoring system performance daily basis ensure adequate response times production applications basic knowledge aws configure ec2 s3 rds skills networking basic knowledge networking operating systems windows xp windows 7 windows 8 linux application software ms office academics mca iftm moradabad uptu lucknow bca dist dhampur mjprubareilly xiith rps thakurdwara u p xth dav sherkot personal details father’s name mrchandrapal singh date birth 30th july 1989 address villalampur disttmoradabad language english hindi marital status single nationality indian hereby declare mentioned information true best knowledge bear responsibility correctness mentioned particulars anuj kumar']\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "\n",
    "parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/others/Anuj_Kumar_Delhi_1.07_yrs.docx\")\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/administrator/Anuj_Kumar_Delhi_1.07_yrs.docx\")\n",
    "\n",
    "#parsedPDF = parser.from_file(\"/home/shabna/Desktop/example_codes/new_sample/level4/tech/IT/administrator/ANISH_GOPI_B_G.doc\")\n",
    "\n",
    "contents=[clean_doc(parsedPDF['content'])]\n",
    "            #print (jd_contents)\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prpInput(pp):\n",
    "    tx = Tokenizer()\n",
    "    tx.fit_on_texts(pp)\n",
    "    vocab_size = len(tx.word_index)+1\n",
    "    encoded_docs = tx.texts_to_sequences(pp)\n",
    "    max_length = 1000\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 48,  49,  18,  19,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "         59,  60,  61,  20,  62,  21,  63,  64,  65,  66,  67,  68,  69,\n",
       "         22,  23,  70,  71,  22,  72,  73,  74,   3,   4,  75,  76,  77,\n",
       "         78,  23,  79,  80,  24,  81,  25,  26,  82,  83,  84,  85,  86,\n",
       "         27,  87,  88,  89,  90,  25,  26,  91,  92,   8,  93,  94,  28,\n",
       "         95,   9,   3,   4,   8,  96,  97,  29,   5,  98,   1,  99, 100,\n",
       "        101, 102,   9, 103, 104,   2,  30, 105, 106, 107, 108, 109, 110,\n",
       "        111, 112, 113, 114,  31, 115,   6,   1, 116,   6,   1, 117, 118,\n",
       "          6,   1, 119,  31,  32,   1, 120, 121,  33,  10, 122, 123,  34,\n",
       "         11,   7,  29, 124,   5, 125, 126, 127, 128,  11,  12, 129, 130,\n",
       "         35, 131,  36,  37,  38,  39,   7, 132, 133, 134, 135,   1, 136,\n",
       "        137, 138, 139,   2, 140, 141, 142, 143,  40,  34, 144, 145, 146,\n",
       "          2, 147, 148,  12, 149,  41, 150, 151,  41,   6,  36,  37,  38,\n",
       "         39,  42,   7, 152,  42,  32,  43,  13, 153, 154, 155,  13, 156,\n",
       "         43, 157, 158,   2,   3,   4, 159,  14,  40, 160,  14,  15,   9,\n",
       "        161,   2, 162, 163, 164, 165,  30, 166,  24,  12, 167,  15,   2,\n",
       "          3,   4,  14,  15,   5, 168, 169, 170, 171, 172, 173, 174,  44,\n",
       "        175,  11, 176,  10, 177,   7, 178,   5,  10, 179,  13, 180,  44,\n",
       "        181, 182, 183, 184, 185,  45,  16, 186,   1, 187, 188, 189,  20,\n",
       "         46,  45,  16,  46, 190, 191,  17, 192,  17, 193,  17, 194,   8,\n",
       "         33,  35, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
       "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218,\n",
       "        219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "        232, 233, 234, 235,  27, 236, 237,  47,  21, 238, 239,  16, 240,\n",
       "         28, 241,  47, 242,  18,  19,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=prpInput(contents)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd = model.predict(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Administrator', 1: 'Others'}\n"
     ]
    }
   ],
   "source": [
    "labels_index = {'Administrator':0 ,'Others':1}\n",
    "rev_lable_index = {}\n",
    "for key in labels_index:\n",
    "    rev_lable_index[labels_index[key]] = key\n",
    "print(rev_lable_index)\n",
    "def result(prd,contents):\n",
    "    y_classes = prd.argmax(axis=-1)\n",
    "    print(len(y_classes))\n",
    "    lx=[]\n",
    "    for idx,lb in enumerate(y_classes):\n",
    "        lx.append([contents[idx],rev_lable_index[lb]])\n",
    "    return lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['curriculum vitae anuj kumar s84a sec52 noida email canujchauhan1gmailcom mobile 91 9871084579 objective seeking position utilize skills abilities information technology industry offers professional growth resourceful innovative certification  training rhcsa  rhce certification number 140201286 certified red hat https wwwredhatcomwappstrainingcertificationverifyhtml  certnumber140201286issearchfalseverifyverify 3months training rhel 60 network nuts new delhi work experience since march 2014 indian council agriculture research icar new delhi till present linux admin roles responsibility installation configuration red hat linux 67 ubuntu nagios monitoring tool configure git shell scripting creating  configuration yum repository managing users  groups acl administration maintenance support samba sftp ftp dns ssh apache nfs server configure apachetomcat server configure squid cache server configure haproxy apache tuning configure awsec2 vpc application system services – lvm data backup nagios iptables monitoring disk space management ensuring data security installing removing software yumrpm’s change ownership permission directory backup tools like tar dump configure smtp postfix dovecot mail managing flexible storage logical volume manager lvm  analyzing storing logs  managing process file security gnupg web hosting  monitor web server change ownership permission directory database backup  restoration database tuning check daily physically equipment maintain daily health check report server’s managing red hat enterprise virtualization manager hypervisor virtualization environment configuration create managing virtual machine templates pools users host network security virtualized environment managing red hat virtualization environment monitoring reports migration high availability establishing documenting procedures ensure integrity data including system failure backup recovery monitoring system performance daily basis ensure adequate response times production applications basic knowledge aws configure ec2 s3 rds skills networking basic knowledge networking operating systems windows xp windows 7 windows 8 linux application software ms office academics mca iftm moradabad uptu lucknow bca dist dhampur mjprubareilly xiith rps thakurdwara u p xth dav sherkot personal details father’s name mrchandrapal singh date birth 30th july 1989 address villalampur disttmoradabad language english hindi marital status single nationality indian hereby declare mentioned information true best knowledge bear responsibility correctness mentioned particulars anuj kumar',\n",
       "  'Administrator']]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result(prd,contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>curriculum vitae anuj kumar s84a sec52 noida e...</td>\n",
       "      <td>Administrator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        File Content          Label\n",
       "0  curriculum vitae anuj kumar s84a sec52 noida e...  Administrator"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result(prd,contents),columns=['File Content','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0]\n",
      " [ 6 20]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(y_test,axis=1),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
